const data = [
  {
    "abstract":
      "Limited capture range and the requirement to provide high quality initializations for optimization-based 2D/3D image registration methods can significantly degrade the per- formance of 3D image reconstruction and motion compensation pipelines. Challenging clinical imaging scenarios, that contain sig- nificant subject motion such as fetal in-utero imaging, complicate the 3D image and volume reconstruction process. In this paper we present a learning based image registra- tion method capable of predicting 3D rigid transformations of arbitrarily oriented 2D image slices, with respect to a learned canonical atlas co-ordinate system. Only image slice intensity information is used to perform registration and canonical align- ment, no spatial transform initialization is required. To find image transformations we utilize a Convolutional Neural Network (CNN) architecture to learn the regression function capable of mapping 2D image slices to the 3D canonical atlas space. We extensively evaluate the effectiveness of our approach quantitatively on simulated Magnetic Resonance Imaging (MRI), fetal brain imagery with synthetic motion and further demon- strate qualitative results on real fetal MRI data where our method is integrated into a full reconstruction and motion compensation pipeline. Our learning based registration achieves an average spatial prediction error of 7 mm on simulated data and produces qualitatively improved reconstructions for heavily moving fetuses with gestational ages of approximately 20 weeks. Our model provides a general and computationally efficient solution to the 2D-3D registration initialization problem and is suitable for real- time scenarios.",
    "authors":
      "Benjamin Hou, Bishesh Khanal, Amir Alansary, Steven McDonagh, Alice Davidson, Mary Rutherford, Jo V. Hajnal, Daniel Rueckert, Ben Glocker, Bernhard Kainz",
    "description": "Retrieved from arXiv. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["bh1511@imperial.ac.uk"],
    "email_new": ["bh1511@imperial.ac.uk"],
    "fullUrl": "https://arxiv.org/abs/1709.06341",
    "key": "b391f338-b950-4f10-bc06-388fc6027782",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1709.06341.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "3d reconstruction in canonical co-ordinate space from arbitrarily oriented 2d images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "b391f338-b950-4f10-bc06-388fc6027782.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "reconstruction",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "fetal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
    "based_on_1_name": "bvlc_googlelenet",
    "based_on_2": "https://github.com/google/inception",
    "based_on_2_name": "google_inception",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "UK",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Image denoising techniques are essential to reducing noise levels and enhancing diagnosis reliability in low-dose computed tomography (CT). Machine learning based denoising methods have shown great potential in removing the complex and spatial-variant noises in CT images. However, some residue artifacts would appear in the denoised image due to complexity of noises. A cascaded training network was proposed in this work, where the trained CNN was applied on the training dataset to initiate new trainings and remove artifacts induced by denoising. A cascades of convolutional neural networks (CNN) were built iteratively to achieve better performance with simple CNN structures. Experiments were carried out on 2016 Low-dose CT Grand Challenge datasets to evaluate the method's performance.",
    "authors": "Dufan Wu, Kyungsang Kim, Georges El Fakhri, Quanzheng Li",
    "description": "9 pages, 9 figures. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["li.quanzheng@mgh.harvard.edu"],
    "email_new": ["li.quanzheng@mgh.harvard.edu"],
    "fullUrl": "https://arxiv.org/abs/1705.04267",
    "key": "eb4f2b27-878e-4057-baf5-d1a748d714b7",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1705.04267.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "a cascaded convolutional neural network for x-ray low-dose ct image denoising",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "eb4f2b27-878e-4057-baf5-d1a748d714b7.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "denoising",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "abdomin",
    "location_1": "0",
    "method_1": "rcnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.aapm.org/grandchallenge/lowdosect/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/cszn/DnCNN",
    "based_on_1_name": "dncnn",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.",
    "authors": "S. Miao,  Z. J. Wang,  R. Liao",
    "description": "IEEE Transactions on Medical Imaging. 2016",
    "doi": "10.1109/TMI.2016.2521800",
    "duplicate": false,
    "email": ["smiao@ece.ubc.ca", "zjwang@ece.ubc.ca", "rui.liao@siemens.com"],
    "email_new": [
      "smiao@ece.ubc.ca",
      "zjwang@ece.ubc.ca",
      "rui.liao@siemens.com"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393571",
    "key": "44acb4cb-d1e0-465f-abc2-9441707db2d3",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "a cnn regression approach for real-time 2d/3d registration",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "44acb4cb-d1e0-465f-abc2-9441707db2d3.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "registration",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "0",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "Canada",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.",
    "authors":
      "N. Kumar,  R. Verma,  S. Sharma,  S. Bhargava,  A. Vahadane,  A. Sethi",
    "description": "IEEE Transactions on Medical Imaging. 2017",
    "doi": "10.1109/TMI.2017.2677499",
    "duplicate": false,
    "email": ["neeraj.kumar.iitg@gmail.com", "amitsethi@gmail.com"],
    "email_new": ["neeraj.kumar.iitg@gmail.com", "amitsethi@gmail.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382",
    "key": "6011dff4-4995-40e3-9827-068f13a3b4e1",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "a dataset and a technique for generalized nuclear segmentation for computational pathology",
    "tools": ["torch"],
    "year": 2017.0,
    "id": "6011dff4-4995-40e3-9827-068f13a3b4e1.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "wsi",
    "data_type_1": "H&E stained",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 1.0,
    "data_link_1": "http://nucleisegmentationbenchmark.weebly.com/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 1.0,
    "code_link": "https://github.com/neerajkumarvaid/NucleiSegmentation",
    "code_ok": 1.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "India",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the Fi score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved Fi score is of up to 0.9955.",
    "authors": "Xiao Jia, Meng MQ.",
    "description": "Conf Proc IEEE Eng Med Biol Soc.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["xjia@ee.cuhk.edu.hk", "qhmeng@ee.cuhk.edu.hk"],
    "email_new": ["xjia@ee.cuhk.edu.hk", "qhmeng@ee.cuhk.edu.hk"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28268409",
    "key": "19fe9d15-1a95-44c4-be60-812ccfb9437c",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "a deep convolutional neural network for bleeding detection in wireless capsule endoscopy images",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "19fe9d15-1a95-44c4-be60-812ccfb9437c.pdf",
    "use": 1,
    "domain": "endoscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "wireless capsule endoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gastrointestinal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "10000",
    "training": 8200.0,
    "validation": 0.0,
    "testing": 1800.0,
    "cross_validation": 0.0,
    "country_1": "Hong Kong",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Epithelial (EP) and stromal (ST) are two types of tissues in histological images. Automated segmentation or classification of EP and ST tissues is important when developing computerized system for analyzing the tumor microenvironment. In this paper, a Deep Convolutional Neural Networks (DCNN) based feature learning is presented to automatically segment or classify EP and ST regions from digitized tumor tissue microarrays (TMAs). Current approaches are based on handcraft feature representation, such as color, texture, and Local Binary Patterns (LBP) in classifying two regions. Compared to handcrafted feature based approaches, which involve task dependent representation, DCNN is an end-to-end feature extractor that may be directly learned from the raw pixel intensity value of EP and ST tissues in a data driven fashion. These high-level features contribute to the construction of a supervised classifier for discriminating the two types of tissues. In this work we compare DCNN based models with three handcraft feature extraction based approaches on two different datasets which consist of 157 Hematoxylin and Eosin (H&amp;E) stained images of breast cancer and 1376 immunohistological (IHC) stained images of colorectal cancer, respectively. The DCNN based feature learning approach was shown to have a F1 classification score of 85%, 89%, and 100%, accuracy (ACC) of 84%, 88%, and 100%, and Matthews Correlation Coefficient (MCC) of 86%, 77%, and 100% on two H&amp;E stained (NKI and VGH) and IHC stained data, respectively. Our DNN based approach was shown to outperform three handcraft feature extraction based approaches in terms of the classification of EP and ST regions.",
    "authors": "Xu J, Luo X, Wang G, Gilmore H, Madabhushi A.",
    "description": "Neurocomputing.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["xujung@gmail.com"],
    "email_new": ["xujung@gmail.com"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28154470",
    "key": "6429810d-0d56-4d3c-8d30-e290517a2495",
    "keyok": 1,
    "keywords":
      "Breast histopathology; Colorectal cancer; Deep Convolutional Neural Networks; Feature representation; The classification of epithelial and stromal regions",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "a deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "6429810d-0d56-4d3c-8d30-e290517a2495.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "segmentation",
    "task_2": "classification",
    "data_type": "wsi",
    "data_type_1": "H&E stained",
    "data_type_2": "IHC stained",
    "location": "breast",
    "location_1": "colorectal",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://tma.im/tma_portal/C-Path/images.html",
    "data_link_2": "http://fimm.webmicroscope.net/supplements/epistroma",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "0",
    "journal": "Neurocomputing",
    "impact_factor": 3.317,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "While cancer is a heterogeneous complex of distinct diseases, the common underlying mechanism for uncontrolled tumor growth is due to mutations in proto-oncogenes and the loss of the regulatory function of tumor suppression genes. In this paper we propose a novel deep learning model for predicting tumor suppression genes (TSGs) and proto-oncogenes (OGs) from their Protein Data Bank (PDB) three dimensional structures. Specifically, we develop a convolutional neural network (CNN) to classify the feature map sets extracted from the tertiary protein structures. Each feature map set represents particular biochemical properties associated with the atomic coordinates appearing on the outer surface of protein's three dimensional structure. The experimental results on the collected dataset for classifying TSGs and OGs demonstrate promising performance with 82.57% accuracy and 0.89 area under the ROC curve. The initial success of the proposed model warrants further study to develop a comprehensive model to identify the cancer driver genes or events using TSG and OG as the basis to track the causal chain.",
    "authors":
      "Amirhossein Tavanaei, Nishanth Anandanadarajah, Rasiah Loganantharaj",
    "description": "Retrieved from bioRxiv. 2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "tavanaei@louisiana.edu",
      "nishanth@louisiana.edu",
      "raja@louisiana.edu"
    ],
    "email_new": [
      "tavanaei@louisiana.edu",
      "nishanth@louisiana.edu",
      "raja@louisiana.edu"
    ],
    "fullUrl": "http://www.biorxiv.org/content/early/2017/08/16/177378",
    "key": "9b35e33a-52c9-4c24-b701-9a52d770bf08",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "http://www.biorxiv.org/content/early/2017/08/16/177378.full.pdf",
    "phase": 1,
    "received": 0,
    "source": "biorxiv",
    "title":
      "a deep learning model for predicting tumor suppressor genes and oncogenes from pdb structure",
    "tools": ["torch"],
    "year": 2017.0,
    "id": "9b35e33a-52c9-4c24-b701-9a52d770bf08.pdf",
    "use": 1,
    "domain": "genomics",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "tertiary protein structure",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://www.rcsb.org/#Category-download",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 1.0,
    "code_link":
      "https://github.com/tavanaei/Cancer-Suppressor-Gene-Deep-Learning",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Multimodal registration is a challenging problem in medical imaging due the high variability of tissue appearance under different imaging modalities. The crucial component here is the choice of the right similarity measure. We make a step towards a general learning-based solution that can be adapted to specific situations and present a metric based on a convolutional neural network. Our network can be trained from scratch even from a few aligned image pairs. The metric is validated on intersubject deformable registration on a dataset different from the one used for training, demonstrating good generalization. In this task, we outperform mutual information by a significant margin.",
    "authors":
      "Martin Simonovsky, Benjam\u00edn Guti\u00e9rrez-Becker, Diana Mateus, Nassir Navab, Nikos Komodakis",
    "description": "Accepted to MICCAI 2016; extended version. 2016",
    "doi": null,
    "duplicate": false,
    "email": [
      "gutierrez.becker@tum.de",
      "martin.simonovsky@enpc.fr",
      "nikos.komodakis@enpc.fr"
    ],
    "email_new": [
      "gutierrez.becker@tum.de",
      "martin.simonovsky@enpc.fr",
      "nikos.komodakis@enpc.fr"
    ],
    "fullUrl": "https://arxiv.org/abs/1609.05396",
    "key": "6469b38f-a01d-4677-a645-959994770605",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1609.05396.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title": "a deep metric for multimodal registration",
    "tools": ["torch"],
    "year": 2016.0,
    "id": "6469b38f-a01d-4677-a645-959994770605.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "registration",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "newborn",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://brain-development.org/ixi-dataset/",
    "data_link_2": "http://brain-development.org/brain-atlases/",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "France",
    "country_2": "Germany",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "Medical Image Computing and Computer-Assisted Intervention",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "With the improvement of people's living standards, there is no doubt that people are paying more and more attention to their health. However, shortage of medical resources is a critical global problem. As a result, an intelligent prognostics system has a great potential to play important roles in computer aided diagnosis. Numerous papers reported that tongue features have been closely related to a human's state. Among them, the majority of the existing tongue image analyses and classification methods are based on the low-level features, which may not provide a holistic view of the tongue. Inspired by a deep convolutional neural network (CNN), we propose a deep tongue image feature analysis system to extract unbiased features and reduce human labor for tongue diagnosis. With the unbalanced sample distribution, it is hard to form a balanced classification model based on feature representations obtained by existing low-level and high-level methods. Our proposed deep tongue image feature analysis model learns high-level features and provide more classification information during training time, which may result in higher accuracy when predicting testing samples. We tested the proposed system on a set of 267 gastritis patients, and a control group of 48 healthy volunteers (labeled according to Western medical practices). Test results show that the proposed deep tongue image feature analysis model can classify a given tongue image into healthy and diseased state with an average accuracy of 91.49%, which demonstrates the relationship between human body's state and its deep tongue image features.",
    "authors":
      "Dan Meng,  Guitao Cao,  Y. Duan,  Minghua Zhu,  Liping Tu,  Jiatuo Xu,  D. Xu",
    "description":
      "2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016",
    "doi": "10.1109/BIBM.2016.7822815",
    "duplicate": false,
    "email": ["samansarraf@ieee.org", "ghi@ryerson.ca"],
    "email_new": ["samansarraf@ieee.org", "ghi@ryerson.ca"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822815",
    "key": "8f553f4f-6705-4e43-be8b-8f1b2cf2857e",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "a deep tongue image features analysis model for medical application",
    "tools": ["digits", "caffe", "convnet"],
    "year": 2016.0,
    "id": "8f553f4f-6705-4e43-be8b-8f1b2cf2857e.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "fmri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "adni.loni.usc.edu",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/NVIDIA/DIGITS/blob/master/digits/standard-networks/caffe/lenet.prototxt",
    "based_on_1_name": "digits_lenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Canada",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2016 Future Technologies Conference (FTC)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automated cardiac segmentation from magnetic resonance imaging datasets is an essential step in the timely diagnosis and management of cardiac pathologies. We propose to tackle the problem of automated left and right ventricle segmentation through the application of a deep fully convolutional neural network architecture. Our model is efficiently trained end-to-end in a single learning stage from whole-image inputs and ground truths to make inference at every pixel. To our knowledge, this is the first application of a fully convolutional neural network architecture for pixel-wise labeling in cardiac magnetic resonance imaging. Numerical experiments demonstrate that our model is robust to outperform previous fully automated methods across multiple evaluation measures on a range of cardiac datasets. Moreover, our model is fast and can leverage commodity compute resources such as the graphics processing unit to enable state-of-the-art cardiac segmentation at massive scales. The models and code are available at this https URL",
    "authors": "Phi Vu Tran",
    "description":
      "Initial Technical Report; Include link to models and code. 2016",
    "doi": null,
    "duplicate": false,
    "email": ["tran_phi@bah.com"],
    "email_new": ["tran_phi@bah.com"],
    "fullUrl": "https://arxiv.org/abs/1604.00494",
    "key": "985c8604-1381-4ebf-85af-bfe32080fb55",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1604.00494.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "a fully convolutional neural network for cardiac segmentation in short-axis mri",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "985c8604-1381-4ebf-85af-bfe32080fb55.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "heart",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.litislab.fr/?projet=1rvsc",
    "data_link_2":
      "http://www.cardiacatlas.org/challenges/lv-segmentation-challenge/",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "https://github.com/vuptran/cardiac-segmentation",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/shelhamer/fcn.berkeleyvision.org",
    "based_on_1_name": "bvlc_fully_convolutional_nets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Medical image fusion technique plays an an increasingly critical role in many clinical applications by deriving the complementary information from medical images with different modalities. In this paper, a medical image fusion method based on convolutional neural networks (CNNs) is proposed. In our method, a siamese convolutional network is adopted to generate a weight map which integrates the pixel activity information from two source images. The fusion process is conducted in a multi-scale manner via image pyramids to be more consistent with human visual perception. In addition, a local similarity based strategy is applied to adaptively adjust the fusion mode for the decomposed coefficients. Experimental results demonstrate that the proposed method can achieve promising results in terms of both visual quality and objective assessment.",
    "authors": "Y. Liu,  X. Chen,  J. Cheng,  H. Peng",
    "description":
      "2017 20th International Conference on Information Fusion (Fusion). 2017",
    "doi": "10.23919/ICIF.2017.8009769",
    "duplicate": false,
    "email": [
      "yuliu@hfut.edu.cn",
      "xun.chen@hfut.edu.cn",
      "chengjuan@hfut.edu.cn",
      "hpeng@hfut.edu.cn"
    ],
    "email_new": [
      "yuliu@hfut.edu.cn",
      "xun.chen@hfut.edu.cn",
      "chengjuan@hfut.edu.cn",
      "hpeng@hfut.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009769",
    "key": "aaa20839-51fc-4026-94cd-04286660dd5d",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "a medical image fusion method based on convolutional neural networks",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "aaa20839-51fc-4026-94cd-04286660dd5d.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "fusion",
    "task_2": "registration",
    "data_type": "ct",
    "data_type_1": "mri",
    "data_type_2": "spect",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.med.harvard.edu/AANLIB/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 20th International Conference on Information Fusion (Fusion)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "BACKGROUND AND OBJECTIVES: Highly accurate classification of biomedical images is an essential task in the clinical diagnosis of numerous medical diseases identified from those images. Traditional image classification methods combined with hand-crafted image feature descriptors and various classifiers are not able to effectively improve the accuracy rate and meet the high requirements of classification of biomedical images. The same also holds true for artificial neural network models directly trained with limited biomedical images used as training data or directly used as a black box to extract the deep features based on another distant dataset. In this study, we propose a highly reliable and accurate end-to-end classifier for all kinds of biomedical images via deep learning and transfer learning. METHODS: e first apply domain transferred deep convolutional neural network for building a deep model; and then develop an overall deep learning architecture based on the raw pixels of original biomedical images using supervised training. In our model, we do not need the manual design of the feature space, seek an effective feature vector classifier or segment specific detection object and image patches, which are the main technological difficulties in the adoption of traditional image classification methods. Moreover, we do not need to be concerned with whether there are large training sets of annotated biomedical images, affordable parallel computing resources featuring GPUs or long times to wait for training a perfect deep model, which are the main problems to train deep neural networks for biomedical image classification as observed in recent works. RESULTS: With the utilization of a simple data augmentation method and fast convergence speed, our algorithm can achieve the best accuracy rate and outstanding classification ability for biomedical images. We have evaluated our classifier on several well-known public biomedical datasets and compared it with several state-of-the-art approaches. CONCLUSIONS: We propose a robust automated end-to-end classifier for biomedical images based on a domain transferred deep convolutional neural network model that shows a highly reliable and accurate performance which has been confirmed on several public biomedical image datasets. Copyright \u00a9 2017 Elsevier Ireland Ltd. All rights reserved.",
    "authors": "Pang S, Yu Z, Orgun MA.",
    "description": "Comput Methods Programs Biomed.  2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "pangshuchao1212@sina.com",
      "yuzz@jlu.edu.cn",
      "mehmet.orgun@mq.edu.au"
    ],
    "email_new": [
      "pangshuchao1212@sina.com",
      "yuzz@jlu.edu.cn",
      "mehmet.orgun@mq.edu.au"
    ],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28254085",
    "key": "633eb771-27f2-45f5-8658-a1c9872aeb48",
    "keyok": 1,
    "keywords":
      "Biomedical image classification; Convolutional neural network; Data augmentation; Deep learning; Transfer learning",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "a novel end-to-end classifier using domain transferred deep convolutional neural networks for biomedical images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "633eb771-27f2-45f5-8658-a1c9872aeb48.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "mri",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "ftp://medical.nema.org/medical/Dicom/Multiframe/",
    "data_link_2": "https://www.oasis-brains.org/",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "Australia",
    "country_3": "0",
    "journal": "Computer Methods and Programs in Biomedicine",
    "impact_factor": 2.503,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "With the increasing ability to routinely and rapidly digitize whole slide images with slide scanners, there has been interest in developing computerized image analysis algorithms for automated detection of disease extent from digital pathology images. The manual identification of presence and extent of breast cancer by a pathologist is critical for patient management for tumor staging and assessing treatment response. However, this process is tedious and subject to inter- and intra-reader variability. For computerized methods to be useful as decision support tools, they need to be resilient to data acquired from different sources, different staining and cutting protocols and different scanners. The objective of this study was to evaluate the accuracy and robustness of a deep learning-based method to automatically identify the extent of invasive tumor on digitized images. Here, we present a new method that employs a convolutional neural network for detecting presence of invasive tumor on whole slide images. Our approach involves training the classifier on nearly 400 exemplars from multiple different sites, and scanners, and then independently validating on almost 200 cases from The Cancer Genome Atlas. Our approach yielded a Dice coefficient of 75.86%, a positive predictive value of 71.62% and a negative predictive value of 96.77% in terms of pixel-by-pixel evaluation compared to manually annotated regions of invasive ductal carcinoma.",
    "authors":
      "Cruz-Roa A, Gilmore H, Basavanhally A, Feldman M, Ganesan S, Shih NNC, Tomaszewski J, Gonz\u00e1lez FA, Madabhushi A.",
    "description": "Sci Rep.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["aacruz@unillanos.edu.co"],
    "email_new": ["aacruz@unillanos.edu.co"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28418027",
    "key": "d1145abc-18c6-4f9f-bf43-f759ef393d3d",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "accurate and reproducible invasive breast cancer detection in whole-slide images: a deep learning approach for quantifying tumor extent",
    "tools": ["torch"],
    "year": 2017.0,
    "id": "d1145abc-18c6-4f9f-bf43-f759ef393d3d.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "wsi",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://portal.gdc.cancer.gov/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Colombia",
    "country_2": "USA",
    "country_3": "0",
    "journal": "Scientific Reports",
    "impact_factor": 4.259,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "High-throughput microscopy of many single cells generates high-dimensional data that are far from straightforward to analyze. One important problem is automatically detecting the cellular compartment where a fluorescently-tagged protein resides, a task relatively simple for an experienced human, but difficult to automate on a computer. Here, we train an 11-layer neural network on data from mapping thousands of yeast proteins, achieving per cell localization classification accuracy of 91%, and per protein accuracy of 99% on held-out images. We confirm that low-level network features correspond to basic image characteristics, while deeper layers separate localization classes. Using this network as a feature calculator, we train standard classifiers that assign proteins to previously unseen compartments after observing only a small number of training examples. Our results are the most accurate subcellular localization classifications to date, and demonstrate the usefulness of deep learning for high-throughput microscopy.Copyright \u00a9 2017 Parnamaa and Parts.",
    "authors": "P\u00e4rnamaa T, Parts L.",
    "description": "G3 (Bethesda).  2017",
    "doi": null,
    "duplicate": false,
    "email": ["leopold.parts@sanger.ac.uk"],
    "email_new": ["leopold.parts@sanger.ac.uk"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28391243",
    "key": "d50796da-87f2-4493-846e-6eeb498acc63",
    "keyok": 1,
    "keywords":
      "deep learning; high-content screening; machine learning; microscopy; yeast",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "accurate classification of protein subcellular localization from high-throughput microscopy images using deep learning",
    "tools": ["tensorflow", "caffe"],
    "year": 2017.0,
    "id": "d50796da-87f2-4493-846e-6eeb498acc63.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "microscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.cs.ut.ee/~leopoldp/2016_DeepYeast",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Estonia",
    "country_2": "UK",
    "country_3": "0",
    "journal": "G3: Genes, Genomes, Genetics",
    "impact_factor": 2.861,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Crying is a way which infants express their needs to their parents. In general, parents often feel worried and anxious when infant crying. For realizing the reason of baby crying, this paper presents an automatic infant crying recognition method. Crying is convert to spectrogram. A convolutional neural networks (CNN) based deep learning is then adopted to train and classify the crying into three categories including hungry, pain, and sleepy. Experimental results shows that the proposed method achieves high classification accuracy.",
    "authors": "C. Y. Chang,  J. J. Li",
    "description":
      "2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW). 2016",
    "doi": "10.1109/ICCE-TW.2016.7520947",
    "duplicate": false,
    "email": ["chuanyu@yuntech.edu.tw"],
    "email_new": ["chuanyu@yuntech.edu.tw"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7520947",
    "key": "bd3122c2-1476-44fe-ad2a-6edfc15b7c68",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "application of deep learning for recognizing infant cries",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "bd3122c2-1476-44fe-ad2a-6edfc15b7c68.pdf",
    "use": 1,
    "domain": "psychology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "audio",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "infant",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Taiwan",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Breast lesion detection using ultrasound imaging is considered an important step of Computer-Aided Diagnosis systems. Over the past decade, researchers have demonstrated the possibilities to automate the initial lesion detection. However, the lack of a common dataset impedes research when comparing the performance of such algorithms. This paper proposes the use of deep learning approaches for breast ultrasound lesion detection and investigates three different methods: a Patch-based LeNet, a U-Net, and a transfer learning approach with a pretrained FCN-AlexNet. Their performance is compared against four state-of-the-art lesion detection algorithms (i.e. Radial Gradient Index, Multifractal Filtering, Rule-based Region Ranking and Deformable Part Models). In addition, this paper compares and contrasts two conventional ultrasound image datasets acquired from two different ultrasound systems. Dataset A comprises 306 (60 malignant and 246 benign) images and Dataset B comprises 163 (53 malignant and 110 benign) images. To overcome the lack of public datasets in this domain, Dataset B will be made available for research purposes. The results demonstrate an overall improvement by the deep learning approaches when assessed on both datasets in terms of True Positive Fraction, False Positives per image, and F-measure.",
    "authors":
      "Yap MH, Pons G, Marti J, Ganau S, Sentis M, Zwiggelaar R, Davison AK, Marti R.",
    "description": "IEEE J Biomed Health Inform.  2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "m.yap@mmu.ac.uk",
      "gponsro@uoc.edu",
      "robert.marti@udg.edu",
      "MSentis@tauli.cat",
      "rrz@aber.ac.uk",
      "adrian.davison@manchester.ac.uk"
    ],
    "email_new": [
      "m.yap@mmu.ac.uk",
      "gponsro@uoc.edu",
      "robert.marti@udg.edu",
      "MSentis@tauli.cat",
      "rrz@aber.ac.uk",
      "adrian.davison@manchester.ac.uk"
    ],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28796627",
    "key": "211d576b-be6a-4ba2-bded-41d6348174e4",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "automated breast ultrasound lesions detection using convolutional neural networks",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "211d576b-be6a-4ba2-bded-41d6348174e4.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "fcn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 1.0,
    "data_link_1": "http://www2.docm.mmu.ac.uk/STAFF/m.yap/dataset.php",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1":
      "https://lmb.informatik.uni-freiburg.de/resources/opensource/unet.en.html",
    "based_on_1_name": "unet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": "469",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "UK",
    "country_2": "Spain",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": "*"
  },
  {
    "abstract":
      "PURPOSE: To develop and test a deep learning approach named Convolutional Neural Network (CNN) for automated screening of T2 -weighted (T2 WI) liver acquisitions for nondiagnostic images, and compare this automated approach to evaluation by two radiologists. MATERIALS AND METHODS: We evaluated 522 liver magnetic resonance imaging (MRI) exams performed at 1.5T and 3T at our institution between November 2014 and May 2016 for CNN training and validation. The CNN consisted of an input layer, convolutional layer, fully connected layer, and output layer. 351\u2009T2 WI were anonymized for training. Each case was annotated with a label of being diagnostic or nondiagnostic for detecting lesions and assessing liver morphology. Another independently collected 171 cases were sequestered for a blind test. These 171\u2009T2 WI were assessed independently by two radiologists and annotated as being diagnostic or nondiagnostic. These 171\u2009T2 WI were presented to the CNN algorithm and image quality (IQ) output of the algorithm was compared to that of two radiologists. RESULTS: There was concordance in IQ label between Reader 1 and CNN in 79% of cases and between Reader 2 and CNN in 73%. The sensitivity and the specificity of the CNN algorithm in identifying nondiagnostic IQ was 67% and 81% with respect to Reader 1 and 47% and 80% with respect to Reader 2. The negative predictive value of the algorithm for identifying nondiagnostic IQ was 94% and 86% (relative to Readers 1 and 2). CONCLUSION: We demonstrate a CNN algorithm that yields a high negative predictive value when screening for nondiagnostic T2 WI of the liver. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2017.\u00a9 2017 International Society for Magnetic Resonance in Medicine.",
    "authors":
      "Esses SJ, Lu X, Zhao T, Shanbhogue K, Dane B, Bruno M, Chandarana H.",
    "description": "J Magn Reson Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["Hersh.Chandarana@nyumc.org"],
    "email_new": ["Hersh.Chandarana@nyumc.org"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28577329",
    "key": "e0207c62-727f-403b-8a69-fb196762732b",
    "keyok": 1,
    "keywords":
      "T2 weighted imaging; convolutional neuronal network; deep learning; image quality; liver MRI; machine learning",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "automated image quality evaluation of t(2) -weighted liver mri utilizing deep learning architecture",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "e0207c62-727f-403b-8a69-fb196762732b.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "quality assesment",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "liver",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Journal of Magnetic Resonance Imaging",
    "impact_factor": 3.083,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, r- spectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.",
    "authors": "L. Yu,  H. Chen,  Q. Dou,  J. Qin,  P. A. Heng",
    "description": "IEEE Transactions on Medical Imaging. 2017",
    "doi": "10.1109/TMI.2016.2642839",
    "duplicate": false,
    "email": [
      "lqyu@cse.cuhk.edu.hk",
      "hchen@cse.cuhk.edu.hk",
      "qdou@cse.cuhk.edu.hk",
      "pheng@cse.cuhk.edu.hk",
      "harry.qin@polyu.edu.hk"
    ],
    "email_new": [
      "lqyu@cse.cuhk.edu.hk",
      "hchen@cse.cuhk.edu.hk",
      "qdou@cse.cuhk.edu.hk",
      "pheng@cse.cuhk.edu.hk",
      "harry.qin@polyu.edu.hk"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792699",
    "key": "0104bacd-b252-4612-b689-cc1c856707b8",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "automated melanoma recognition in dermoscopy images via very deep residual networks",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "0104bacd-b252-4612-b689-cc1c856707b8.pdf",
    "use": 1,
    "domain": "dermatology",
    "task_1": "segmentation",
    "task_2": "classification",
    "data_type": "dermoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "skin",
    "location_1": "0",
    "method_1": "fcrn",
    "method_2": "cnn",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://isic-archive.com/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 1.0,
    "code_link": "https://github.com/yulequan/melanoma-recognition",
    "code_ok": 1.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "1250",
    "training": 900.0,
    "validation": 0.0,
    "testing": 350.0,
    "cross_validation": 0.0,
    "country_1": "Hong Kong",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE TRANSACTIONS ON MEDICAL IMAGING",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "We present a computer-aided diagnosis system (CADx) for the automatic categorization of solid, part-solid and non-solid nodules in pulmonary computerized tomography images using a Convolutional Neural Network (CNN). Provided with only a two-dimensional region of interest (ROI) surrounding each nodule, our CNN automatically reasons from image context to discover informative computational features. As a result, no image segmentation processing is needed for further analysis of nodule attenuation, allowing our system to avoid potential errors caused by inaccurate image processing. We implemented two computerized texture analysis schemes, classification and regression, to automatically categorize solid, part-solid and non-solid nodules in CT scans, with hierarchical features in each case learned directly by the CNN model. To show the effectiveness of our CNN-based CADx, an established method based on histogram analysis (HIST) was implemented for comparison. The experimental results show significant performance improvement by the CNN model over HIST in both classification and regression tasks, yielding nodule classification and rating performance concordant with those of practicing radiologists. Adoption of CNN-based CADx systems may reduce the inter-observer variation among screening radiologists and provide a quantitative reference for further nodule analysis.",
    "authors":
      "Tu X, Xie M, Gao J, Ma Z, Chen D, Wang Q, Finlayson SG, Ou Y, Cheng JZ.",
    "description": "Sci Rep.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["mxie@uestc.edu.cn", "jzcheng@ntu.edu.tw"],
    "email_new": ["mxie@uestc.edu.cn", "jzcheng@ntu.edu.tw"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28864824",
    "key": "81d6c0eb-9cc3-4cb0-83d9-c67b86ef8c1c",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "automatic categorization and scoring of solid, part-solid and non-solid pulmonary nodules in ct images with convolutional neural network",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "81d6c0eb-9cc3-4cb0-83d9-c67b86ef8c1c.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "regression",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "lung",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "0",
    "journal": "Scientific Reports",
    "impact_factor": 4.259,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.",
    "authors": "Zhang R, Zheng Y, Mak TW, Yu R, Wong SH, Lau JY, Poon CC.",
    "description": "IEEE J Biomed Health Inform.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["cpoon@surgery.cuhk.edu.hk"],
    "email_new": ["cpoon@surgery.cuhk.edu.hk"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28114040",
    "key": "3bd582af-d98e-4f9e-8985-040bca4e4925",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "automatic detection and classification of colorectal polyps by transferring low-level cnn features from nonmedical domain",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "3bd582af-d98e-4f9e-8985-040bca4e4925.pdf",
    "use": 1,
    "domain": "colonoscopy",
    "task_1": "detection",
    "task_2": "classification",
    "data_type": "colonoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "colon",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.depeca.uah.es/colonoscopy_dataset/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
    "based_on_1_name": "bvlc_caffenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": "291",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "Hong Kong",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Considerable practical interest exists in being able to automatically determine whether a recorded magnetic resonance image is affected by motion artifacts caused by patient movements during scanning. Existing approaches usually rely on the use of navigators or external sensors to detect and track patient motion during image acquisition. In this work, we present an algorithm based on convolutional neural networks that enables fully automated detection of motion artifacts in MR scans without special hardware requirements. The approach is data driven and uses the magnitude of MR images in the spatial domain as input. We evaluate the performance of our algorithm on both synthetic and real data and observe adequate performance in terms of accuracy and generalization to different types of data. Our proposed approach could potentially be used in clinical practice to tag an MR image as motion-free or motion-corrupted immediately after a scan is finished. This process would facilitate the acquisition of high-quality MR images that are often indispensable for accurate medical diagnosis.",
    "authors": "K. Meding,  A. Loktyushin,  M. Hirsch",
    "description":
      "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017",
    "doi": "10.1109/ICASSP.2017.7952268",
    "duplicate": false,
    "email": [
      "KristoJ.Meding@tuebingen.mpg.de",
      "Alexander.Loktyushin@tuebingen.mpg.de",
      "Michael.Hirsch@tuebingen.mpg.de"
    ],
    "email_new": [
      "KristoJ.Meding@tuebingen.mpg.de",
      "Alexander.Loktyushin@tuebingen.mpg.de",
      "Michael.Hirsch@tuebingen.mpg.de"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952268",
    "key": "d93b13b9-b5c3-4c7c-8334-397c791aca87",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "automatic detection of motion artifacts in mr images using cnns",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "d93b13b9-b5c3-4c7c-8334-397c791aca87.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Germany",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.",
    "authors": "Y. Li,  W. Liang,  Y. Zhang,  H. An,  J. Tan",
    "description":
      "2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016",
    "doi": "10.1109/EMBC.2016.7590785",
    "duplicate": false,
    "email": ["liyang@sia.cn", "liang-wei2014315@gmail.com", "tan@utk.edu"],
    "email_new": ["liyang@sia.cn", "liang-wei2014315@gmail.com", "tan@utk.edu"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590785",
    "key": "9ccddf12-0caa-4122-8bc2-a9f56a370bde",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "automatic lumbar vertebrae detection based on feature fusion deep learning for partial occluded c-arm x-ray images",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "9ccddf12-0caa-4122-8bc2-a9f56a370bde.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "spine",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://www.microsoft.com/en-us/research/project/medical-image-analysis/?from=http%3A%2F%2Fresearch.microsoft.com%2Fprojects%2Fmedicalimageanalysis%2F",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 \u00b1 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views.",
    "authors":
      "Abdi AH, Luong C, Tsang T, Allan G, Nouranian S, Jue J, Hawley D, Fleming S, Gin K, Swift J, Rohling R, Abolmaesumi P.",
    "description": "IEEE Trans Med Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["purang@ece.ubc.ca"],
    "email_new": ["purang@ece.ubc.ca"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28391191",
    "key": "3db1dca1-3ee6-416e-ac50-74359d00efb7",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "automatic quality assessment of echocardiograms using convolutional neural networks: feasibility on the apical four-chamber view",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "3db1dca1-3ee6-416e-ac50-74359d00efb7.pdf",
    "use": 1,
    "domain": "echocardiography",
    "task_1": "quality assesment",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "heart",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "6,916",
    "training": 5533.0,
    "validation": 0.0,
    "testing": 1383.0,
    "cross_validation": 1.0,
    "country_1": "Canada",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Counting bacterial colonies on microbiological culture plates is a time-consuming, error-prone, nevertheless fundamental task in microbiology. Computer vision based approaches can increase the efficiency and the reliability of the process, but accurate counting is challenging, due to the high degree of variability of agglomerated colonies. In this paper, we propose a solution which adopts Convolutional Neural Networks (CNN) for counting the number of colonies contained in confluent agglomerates, that scored an overall accuracy of the 92.8% on a large challenging dataset. The proposed CNN-based technique for estimating the cardinality of colony aggregates outperforms traditional image processing approaches, becoming a promising approach to many related applications.",
    "authors": "Ferrari A, Lombardi S, Signoroni A.",
    "description": "Conf Proc IEEE Eng Med Biol Soc.  2015",
    "doi": null,
    "duplicate": false,
    "email": [
      "alberto.signoroni@unibs.it",
      "alessandro.ferrari@copanitalia.com"
    ],
    "email_new": [
      "alberto.signoroni@unibs.it",
      "alessandro.ferrari@copanitalia.com"
    ],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/26738016",
    "key": "d077a6b8-a563-41c7-aa51-72f3d58ee930",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title": "bacterial colony counting by convolutional neural networks",
    "tools": ["caffe"],
    "year": 2015.0,
    "id": "d077a6b8-a563-41c7-aa51-72f3d58ee930.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "counting",
    "task_2": "classification",
    "data_type": "microscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt",
    "based_on_1_name": "bvlc_lenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Italy",
    "country_2": "0",
    "country_3": "0",
    "journal": "pattern recognition",
    "impact_factor": 3.399,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The complex language of eukaryotic gene expression remains incompletely understood. Despite the importance suggested by many noncoding variants statistically associated with human disease, nearly all such variants have unknown mechanism. Here, we address this challenge using an approach based on a recent machine learning advance--deep convolutional neural networks (CNNs). We introduce an open source package Basset (https://github.com/davek44/Basset) to apply CNNs to learn the functional activity of DNA sequences from genomics data. We trained Basset on a compendium of accessible genomic sites mapped in 164 cell types by DNaseI-seq and demonstrate far greater predictive accuracy than previous methods. Basset predictions for the change in accessibility between variant alleles were far greater for GWAS SNPs that are likely to be causal relative to nearby SNPs in linkage disequilibrium with them. With Basset, a researcher can perform a single sequencing assay in their cell type of interest and simultaneously learn that cell's chromatin accessibility code and annotate every mutation in the genome with its influence on present accessibility and latent potential for accessibility. Thus, Basset offers a powerful computational approach to annotate and interpret the noncoding genome.",
    "authors": "David R Kelley, Jasper Snoek, John Rinn",
    "description": "Now published in Genome Research. 2016",
    "doi": null,
    "duplicate": false,
    "email": [
      "dkelley@fas.harvard.edu",
      "jsnoek@seas.harvard.edu",
      "john_rinn@harvard.edu"
    ],
    "email_new": [
      "dkelley@fas.harvard.edu",
      "jsnoek@seas.harvard.edu",
      "john_rinn@harvard.edu"
    ],
    "fullUrl": "http://www.biorxiv.org/content/early/2016/02/18/028399",
    "key": "7ba25133-06c5-4c75-b2d4-e62dc4c8c250",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "http://www.biorxiv.org/content/early/2016/02/18/028399.full.pdf",
    "phase": 1,
    "received": 0,
    "source": "biorxiv",
    "title":
      "basset: learning the regulatory code of the accessible genome with deep convolutional neural networks",
    "tools": ["digits", "torch"],
    "year": 2016.0,
    "id": "7ba25133-06c5-4c75-b2d4-e62dc4c8c250.pdf",
    "use": 1,
    "domain": "genomics",
    "task_1": "annotation",
    "task_2": "0",
    "data_type": "dna",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://genome.ucsc.edu/",
    "data_link_2": "http://www.roadmapepigenomics.org/",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "https://github.com/davek44/Basset",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Genome Research",
    "impact_factor": 11.351,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The task of MRI fingerprinting is to identify tissue parameters from complex-valued MRI signals. The prevalent approach is dictionary based, where a test MRI signal is compared to stored MRI signals with known tissue parameters and the most similar signals and tissue parameters retrieved. Such an approach does not scale with the number of parameters and is rather slow when the tissue parameter space is large. Our first novel contribution is to use deep learning as an efficient nonlinear inverse mapping approach. We generate synthetic (tissue, MRI) data from an MRI simulator, and use them to train a deep net to map the MRI signal to the tissue parameters directly. Our second novel contribution is to develop a complex-valued neural network with new cardioid activation functions. Our results demonstrate that complex-valued neural nets could be much more accurate than real-valued neural nets at complex-valued MRI fingerprinting.",
    "authors": "Patrick Virtue, Stella X. Yu, Michael Lustig",
    "description":
      "Accepted in Proc. IEEE International Conference on Image Processing (ICIP), 2017. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["virtue@berkeley.edu"],
    "email_new": ["virtue@berkeley.edu"],
    "fullUrl": "https://arxiv.org/abs/1707.00070",
    "key": "2b27e52f-7df6-4b60-9ed8-5900da98cb2c",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1707.00070.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "better than real: complex-valued neural nets for mri fingerprinting",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "2b27e52f-7df6-4b60-9ed8-5900da98cb2c.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "fingerprinting",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "0",
    "location_1": "0",
    "method_1": "fullyconnectednn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 0.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "0",
    "training": 100000.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      " IEEE International Conference on Image Processing (ICIP), 2017",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      " Gliomas are rapidly progressive, neurologically devastating, largely fatal brain tumors. Magnetic resonance imaging (MRI) is a widely used technique employed in the diagnosis and management of gliomas in clinical practice. MRI is also the standard imaging modality used to delineate the brain tumor target as part of treatment planning for the administration of radiation therapy. Despite more than 20 yr of research and development, computational brain tumor segmentation in MRI images remains a challenging task. We are presenting a novel method of automatic image segmentation based on holistically nested neural networks that could be employed for brain tumor segmentation of MRI images. METHODS: Two preprocessing techniques were applied to MRI images. The N4ITK method was employed for correction of bias field distortion. A novel landmark-based intensity normalization method was developed so that tissue types have a similar intensity scale in images of different subjects for the same MRI protocol. The holistically nested neural networks (HNN), which extend from the convolutional neural networks (CNN) with a deep supervision through an additional weighted-fusion output layer, was trained to learn the multiscale and multilevel hierarchical appearance representation of the brain tumor in MRI images and was subsequently applied to produce a prediction map of the brain tumor on test images. Finally, the brain tumor was obtained through an optimum thresholding on the prediction map. RESULTS: The proposed method was evaluated on both the Multimodal Brain Tumor Image Segmentation (BRATS) Benchmark 2013 training datasets, and clinical data from our institute. A dice similarity coefficient (DSC) and sensitivity of 0.78 and 0.81 were achieved on 20 BRATS 2013 training datasets with high-grade gliomas (HGG), based on a two-fold cross-validation. The HNN model built on the BRATS 2013 training data was applied to ten clinical datasets with HGG from a locally developed database. DSC and sensitivity of 0.83 and 0.85 were achieved. A quantitative comparison indicated that the proposed method outperforms the popular fully convolutional network (FCN) method. In terms of efficiency, the proposed method took around 10 h for training with 50,000 iterations, and approximately 30 s for testing of a typical MRI image in the BRATS 2013 dataset with a size of 160 \u00d7 216 \u00d7 176, using a DELL PRECISION workstation T7400, with an NVIDIA Tesla K20c GPU.CONCLUSIONS: An effective brain tumor segmentation method for MRI images based on a HNN has been developed. The high level of accuracy and efficiency make this method practical in brain tumor segmentation. It may play a crucial role in both brain tumor diagnostic analysis and in the treatment planning of radiation therapy. Published 2017. This article is a U.S. Government work and is in the public domain in the USA.",
    "authors":
      "Zhuge Y, Krauze AV, Ning H, Cheng JY, Arora BC, Camphausen K, Miller RW.",
    "description": "Med Phys.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["zhugey@nih.gov"],
    "email_new": ["zhugey@nih.gov"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28736864",
    "key": "ac01437e-df0b-45c8-af27-0f7f943a8a44",
    "keyok": 1,
    "keywords":
      "MRI image; brain tumor; convolutional neural networks; holistically nested neural networks; image segmentation",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "brain tumor segmentation using holistically nested neural networks in mri images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "ac01437e-df0b-45c8-af27-0f7f943a8a44.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "hnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://www.smir.ch/BRATS/Start2013#download",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/s9xie/hed",
    "based_on_1_name": "holistically_nested_edge_detection",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "medical physics",
    "impact_factor": 2.635,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automated breast cancer multi-classification from histopathological images plays a key role in computer-aided breast cancer diagnosis or prognosis. Breast cancer multi-classification is to identify subordinate classes of breast cancer (Ductal carcinoma, Fibroadenoma, Lobular carcinoma, etc.). However, breast cancer multi-classification from histopathological images faces two main challenges from: (1) the great difficulties in breast cancer multi-classification methods contrasting with the classification of binary classes (benign and malignant), and (2) the subtle differences in multiple classes due to the broad variability of high-resolution image appearances, high coherency of cancerous cells, and extensive inhomogeneity of color distribution. Therefore, automated breast cancer multi-classification from histopathological images is of great clinical significance yet has never been explored. Existing works in literature only focus on the binary classification but do not support further breast cancer quantitative assessment. In this study, we propose a breast cancer multi-classification method using a newly proposed deep learning model. The structured deep learning model has achieved remarkable performance (average 93.2% accuracy) on a large-scale dataset, which demonstrates the strength of our method in providing an efficient tool for breast cancer multi-classification in clinical settings.",
    "authors": "Han Z, Wei B, Zheng Y, Yin Y, Li K, Li S.",
    "description": "Sci Rep.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["wbz99@sina.com"],
    "email_new": ["wbz99@sina.com"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28646155",
    "key": "445e091e-d8a7-4e31-a501-26eaf91c882c",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "breast cancer multi-classification from histopathological images with structured deep learning model",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "445e091e-d8a7-4e31-a501-26eaf91c882c.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "histopathology",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": "82",
    "training": 61.0,
    "validation": 0.0,
    "testing": 21.0,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "UK",
    "country_3": "0",
    "journal": "nature scientific reports",
    "impact_factor": 4.259,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Hyperspectral imaging is a rising imaging modality in the field of medical applications, and the combination of both spectral and spatial information provides wealth information for cell classification. In this paper, deep convolutional neural network (CNN) is employed to achieve blood cell discrimination in medical hyperspectral images (MHSI). As a deep learning architecture, CNNs are expected to get more discriminative and semantic features, which effect classification accuracy to a certain extent. Experimental results based on two real medical hyperspectral image data sets demonstrate that cell classification using CNNs is effective. In addition, compared to traditional support vector machine (SVM), the proposed method, which jointly exploits spatial and spectral features, can achieve better classification performance, showcasing the CNN-based methods' tremendous potential for accurate medical hyperspectral data classification.",
    "authors": "Xiang Li,  W. Li,  Xiaodong Xu,  Wei Hu",
    "description":
      "2017 2nd International Conference on Image, Vision and Computing (ICIVC). 2017",
    "doi": "10.1109/ICIVC.2017.7984606",
    "duplicate": false,
    "email": ["liwei089@ieee.org"],
    "email_new": ["liwei089@ieee.org"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984606",
    "key": "310fb8e5-7fda-416b-84c0-35cb33685984",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "cell classification using convolutional neural networks in medical hyperspectral imagery",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "310fb8e5-7fda-416b-84c0-35cb33685984.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "hsi",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "blood",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "24222",
    "training": 9687.0,
    "validation": 0.0,
    "testing": 14535.0,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 2nd International Conference on Image, Vision and Computing",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Dental records play an important role in forensic identification. To this end, postmortem dental findings and teeth conditions are recorded in a dental chart and compared with those of antemortem records. However, most dentists are inexperienced at recording the dental chart for corpses, and it is a physically and mentally laborious task, especially in large scale disasters. Our goal is to automate the dental filing process by using dental x-ray images. In this study, we investigated the application of a deep convolutional neural network (DCNN) for classifying tooth types on dental cone-beam computed tomography (CT) images. Regions of interest (ROIs) including single teeth were extracted from CT slices. Fifty two CT volumes were randomly divided into 42 training and 10 test cases, and the ROIs obtained from the training cases were used for training the DCNN. For examining the sampling effect, random sampling was performed 3 times, and training and testing were repeated. We used the AlexNet network architecture provided in the Caffe framework, which consists of 5 convolution layers, 3 pooling layers, and 2 full connection layers. For reducing the overtraining effect, we augmented the data by image rotation and intensity transformation. The test ROIs were classified into 7 tooth types by the trained network. The average classification accuracy using the augmented training data by image rotation and intensity transformation was 88.8%. Compared with the result without data augmentation, data augmentation resulted in an approximately 5% improvement in classification accuracy. This indicates that the further improvement can be expected by expanding the CT dataset. Unlike the conventional methods, the proposed method is advantageous in obtaining high classification accuracy without the need for precise tooth segmentation. The proposed tooth classification method can be useful in automatic filing of dental charts for forensic identification.Copyright \u00a9 2016 Elsevier Ltd. All rights reserved.",
    "authors":
      "Miki Y, Muramatsu C, Hayashi T, Zhou X, Hara T, Katsumata A, Fujita H.",
    "description": "Comput Biol Med.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["chisa@fjt.info.gifu-u.ac.jp"],
    "email_new": ["chisa@fjt.info.gifu-u.ac.jp"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27889430",
    "key": "5d20cbe9-c3f0-41a5-9084-f623c01a9258",
    "keyok": 1,
    "keywords":
      "Deep convolutional neural networks; Dental chart; Dental cone-beam CT; Forensic identification; Tooth classification",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "classification of teeth in cone-beam ct using deep convolutional neural network",
    "tools": ["digits", "caffe"],
    "year": 2017.0,
    "id": "5d20cbe9-c3f0-41a5-9084-f623c01a9258.pdf",
    "use": 1,
    "domain": "dentistry",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "teeth",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Japan",
    "country_2": "0",
    "country_3": "0",
    "journal": "Computers in Biology and Medicine",
    "impact_factor": 1.836,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-toimage weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images.",
    "authors": "Jia Z, Huang X, Chang EI, Xu Y.",
    "description": "IEEE Trans Med Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "huangxingyi102@126.com",
      "xuyan04@gmail.com",
      "zhipeng.jia@outlook.com",
      "echang@microsoft.com",
      "xuyan04@gmail.com",
      "zhipeng.jia@outlook.com"
    ],
    "email_new": [
      "huangxingyi102@126.com",
      "xuyan04@gmail.com",
      "zhipeng.jia@outlook.com",
      "echang@microsoft.com",
      "xuyan04@gmail.com",
      "zhipeng.jia@outlook.com"
    ],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28692971",
    "key": "bfffca90-88eb-4036-91e5-45058f85c96e",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "constrained deep weak supervision for histopathology image segmentation",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "bfffca90-88eb-4036-91e5-45058f85c96e.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "histapathology",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "colon",
    "method_1": "fullycnn",
    "method_2": "cnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2":
      "https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/",
    "based_on_2_name": "unet",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "MOTIVATION: Convolutional neural networks (CNN) have outperformed conventional methods in modeling the sequence specificity of DNA-protein binding. Yet inappropriate CNN architectures can yield poorer performance than simpler models. Thus an in-depth understanding of how to match CNN architecture to a given task is needed to fully harness the power of CNNs for computational biology applications. RESULTS: We present a systematic exploration of CNN architectures for predicting DNA sequence binding using a large compendium of transcription factor datasets. We identify the best-performing architectures by varying CNN width, depth and pooling designs. We find that adding convolutional kernels to a network is important for motif-based tasks. We show the benefits of CNNs in learning rich higher-order sequence features, such as secondary motifs and local sequence context, by comparing network performance on multiple modeling tasks ranging in difficulty. We also demonstrate how careful construction of sequence benchmark datasets, using approaches that control potentially confounding effects like positional or motif strength bias, is critical in making fair comparisons between competing methods. We explore how to establish the sufficiency of training data for these learning tasks, and we have created a flexible cloud-based framework that permits the rapid exploration of alternative neural network architectures for problems in computational biology. AVAILABILITY AND IMPLEMENTATION: All the models analyzed are available at http://cnn.csail.mit.edu. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.\u00a9 The Author 2016. Published by Oxford University Press.",
    "authors": "Zeng H, Edwards MD, Liu G, Gifford DK.",
    "description": "Bioinformatics.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["gifford@mit.edu"],
    "email_new": ["gifford@mit.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27307608",
    "key": "6f605dcd-ac54-411e-8a52-73eb0886510e",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "convolutional neural network architectures for predicting dna-protein binding",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "6f605dcd-ac54-411e-8a52-73eb0886510e.pdf",
    "use": 1,
    "domain": "genomics",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "dna",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeAwgTfbsUniform/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "http://cnn.csail.mit.edu/",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Bioinformatics",
    "impact_factor": 7.307,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Diagnosis and cure of cancer has been one of the biggest challenges faced by mankind in the last few decades. Early detection of cancer would facilitate in saving millions of lives across the globe every year. This paper presents an approach which uses a Convolutional Neural Network (CNNs) to classify tumours seen in lung cancer screening computed tomography scans as malignant or benign. CNNs have special properties such as spatial invariance, and allow for multiple feature extraction. When such layers are cascaded, leading to Deep CNNs, it has been shown widely that the accuracy of prediction increases dramatically. In this work, we have designed a CNN suitable for the analysis of CT scans with tumours, using domain knowledge from both medicine and neural networks. The results show that the accuracy of classification for our network performs better than both the traidtional neural networks, and also existing CNNs built for image classification purposes.",
    "authors": "P. Rao,  N. A. Pereira,  R. Srinivasan",
    "description":
      "2016 2nd International Conference on Contemporary Computing and Informatics (IC3I). 2016",
    "doi": "10.1109/IC3I.2016.7918014",
    "duplicate": false,
    "email": [
      "prajwaljpj@gmail.com",
      "nishalpereira@gmail.com",
      "raghuram@msrit.edu"
    ],
    "email_new": [
      "prajwaljpj@gmail.com",
      "nishalpereira@gmail.com",
      "raghuram@msrit.edu"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918014",
    "key": "c642c7a5-cd18-4525-b3f7-1f2a103a7765",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "convolutional neural networks for lung cancer screening in computed tomography (ct) scans",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "c642c7a5-cd18-4525-b3f7-1f2a103a7765.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "lung",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt",
    "based_on_1_name": "bvlc_lenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "India",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 2nd International Conference on Contemporary Computing and Informatics (IC3I)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "For automatic screening of eye diseases, it is very important to segment regions corresponding to the different eye-parts from the fundal images. A challenging task, in this context, is to segment the network of blood vessels. The blood vessel network runs all along the fundal image, varying in density and fineness of structure. Besides, changes in illumination, color and pathology also add to the difficulties in blood vessel segmentation. In this paper, we propose segmentation of blood vessels from fundal images in the deep learning framework, without any pre-processing. A deep convolutional network, consisting of 8 convolutional layers and 3 pooling layers in between, is used to achieve the segmentation. In this work, a Convolutional Neural Network currently in use for semantic image segmentation is customized for blood vessel segmentation by replacing the output layer with a convolutional layer of kernel size 1 \u00d7 1 which generates the final segmented image. The output of CNN is a gray scale image and is binarized by thresholding. The proposed method is applied on 2 publicly available databases DRIVE and HRF (capturing diversity in image resolution), consisting of healthy and diseased fundal images boosted by mirror versions of the originals. The method results in an accuracy of 93.94% and yields 0.894 as area under the ROC curve on the test data comprising of randomly selected 23 images from HRF dataset. The promising results illustrate generalizability of the proposed approach.",
    "authors": "S. K. Vengalil,  N. Sinha,  S. S. S. Kruthiventi,  R. V. Babu",
    "description":
      "2016 International Conference on Signal Processing and Communications (SPCOM). 2016",
    "doi": "10.1109/SPCOM.2016.7746702",
    "duplicate": false,
    "email": [
      "sunilkumar.vengalil@iiitb.org",
      "neelam.sinha@iiitb.ac.in",
      "kssaisrinivas@gmail.com",
      "venky@cds.iisc.ac.in"
    ],
    "email_new": [
      "sunilkumar.vengalil@iiitb.org",
      "neelam.sinha@iiitb.ac.in",
      "kssaisrinivas@gmail.com",
      "venky@cds.iisc.ac.in"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746702",
    "key": "afa5c419-2eb3-41b6-b4d3-8cd23d8c6c45",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "customizing cnns for blood vessel segmentation from fundus images",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "afa5c419-2eb3-41b6-b4d3-8cd23d8c6c45.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "fundus photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://www5.cs.fau.de/research/data/fundus-images/",
    "data_link_2": "http://www.isi.uu.nl/Research/Databases/DRIVE/",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "https://bitbucket.org/deeplab/deeplab-public/",
    "based_on_2_name": "deeplab",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "India",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 International Conference on Signal Processing and Communications (SPCOM)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.",
    "authors": "H. Chen,  X. Qi,  L. Yu,  P. A. Heng",
    "description":
      "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016",
    "doi": "10.1109/CVPR.2016.273",
    "duplicate": false,
    "email": [
      "hchen@cse.cuhk.edu.hk",
      "xjqi@cse.cuhk.edu.hk",
      "lqyu@cse.cuhk.edu.hk",
      "pheng@cse.cuhk.edu.hk"
    ],
    "email_new": [
      "hchen@cse.cuhk.edu.hk",
      "xjqi@cse.cuhk.edu.hk",
      "lqyu@cse.cuhk.edu.hk",
      "pheng@cse.cuhk.edu.hk"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780642",
    "key": "717f3d26-d55a-44da-8e6b-27621e1a3266",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "dcan: deep contour-aware networks for accurate gland segmentation",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "717f3d26-d55a-44da-8e6b-27621e1a3266.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "histapathology",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gland",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/tensorflow/models/tree/master/research/deeplab",
    "based_on_1_name": "deeplab",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Hong Kong",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE Conference on Computer Vision and Pattern Recognition",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "INTRODUCTION: To develop real-time image processing for image-guided radiotherapy, we evaluated several neural network models for use with different imaging modalities, including X-ray fluoroscopic image denoising. MATERIALS: Setup images of prostate cancer patients were acquired with two oblique X-ray fluoroscopic units. Two types of residual network were designed: a convolutional autoencoder (rCAE) and a convolutional neural network (rCNN). We changed the convolutional kernel size and number of convolutional layers for both networks, and the number of pooling and upsampling layers for rCAE. The ground-truth image was applied to the contrast-limited adaptive histogram equalization (CLAHE) method of image processing. Network models were trained to keep the quality of the output image close to that of the ground-truth image from the input image without image processing. For image denoising evaluation, noisy input images were used for the training. RESULTS: More than 6 convolutional layers with convolutional kernels &gt;5\u00d75 improved image quality. However, this did not allow real-time imaging. After applying a pair of pooling and upsampling layers to both networks, rCAEs with &gt;3 convolutions each and rCNNs with &gt;12 convolutions with a pair of pooling and upsampling layers achieved real-time processing at 30 frames per second (fps) with acceptable image quality. CONCLUSIONS: Use of our suggested network achieved real-time image processing for contrast enhancement and image denoising by the use of a conventional modern personal computer. Copyright \u00a9 2017 Associazione Italiana di Fisica Medica. Published by Elsevier Ltd. All rights reserved.",
    "authors": "Mori S.",
    "description": "Phys Med.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["mori.shinichiro@qst.go.jp"],
    "email_new": ["mori.shinichiro@qst.go.jp"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28743618",
    "key": "3b168276-17e1-4dc7-91e9-d6dcb969b3e6",
    "keyok": 1,
    "keywords":
      "Computer-assisted; Fluoroscopy; Image processing; Neural network model; Radiation therapy",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "deep architecture neural network-based real-time image processing for image-guided radiotherapy",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "3b168276-17e1-4dc7-91e9-d6dcb969b3e6.pdf",
    "use": 1,
    "domain": "image-guided radiotherapy",
    "task_1": "denoising",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "prostate",
    "location_1": "0",
    "method_1": "rcae",
    "method_2": "rcnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "42",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 0.0,
    "country_1": "Japan",
    "country_2": "0",
    "country_3": "0",
    "journal": "Physica Medica",
    "impact_factor": 1.99,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.",
    "authors": "Y. Xu,  Z. Jia,  Y. Ai,  F. Zhang,  M. Lai,  E. I. C. Chang",
    "description":
      "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2015",
    "doi": "10.1109/ICASSP.2015.7178109",
    "duplicate": false,
    "email": [
      "xuyan04@gmail.com",
      "lmd@zju.edu.cn",
      "eric.chang@microsoft.com",
      "v-zhijia@microsoft.com",
      "v-yuai@microsoft.com",
      "v-fangz@microsoft.com"
    ],
    "email_new": [
      "xuyan04@gmail.com",
      "lmd@zju.edu.cn",
      "eric.chang@microsoft.com",
      "v-zhijia@microsoft.com",
      "v-yuai@microsoft.com",
      "v-fangz@microsoft.com"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178109",
    "key": "6a3fa06b-cd73-4ee2-94aa-942ee6a5a28a",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "deep convolutional activation features for large scale brain tumor histopathology image classification and segmentation",
    "tools": ["caffe"],
    "year": 2015.0,
    "id": "6a3fa06b-cd73-4ee2-94aa-942ee6a5a28a.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "segmentation",
    "data_type": "histapathology",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://cancergenome.nih.gov/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",
    "based_on_1_name": "alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2015 IEEE International Conference on Acoustics, Speech and Signal Processing",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "We propose a fully automatic method for segmenting myelin and axon from microscopy images of excised mouse spinal cord based on Convolutional Neural Networks (CNNs) and Deep Convolutional Encoder-Decoder. We compare a two-class CNN, multi-class CNN, and multi-class deep convolutional encoder-decoder with traditional methods. The CNN method gives a pixel-wise accuracy of 79.7% whereas an Active Contour method gives 59.4%. The encoder-decoder shows better performance with 82.3% and noticeably shorter classification time than CNN methods.",
    "authors": "R. Mesbah,  B. McCane,  S. Mills",
    "description":
      "2016 International Conference on Image and Vision Computing New Zealand (IVCNZ). 2016",
    "doi": "10.1109/IVCNZ.2016.7804455",
    "duplicate": false,
    "email": [
      "rassoul@cs.otago.ac.nz",
      "mccane@cs.otago.ac.nz",
      "steven@cs.otago.ac.nz"
    ],
    "email_new": [
      "rassoul@cs.otago.ac.nz",
      "mccane@cs.otago.ac.nz",
      "steven@cs.otago.ac.nz"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804455",
    "key": "04327d42-c350-4022-901a-6e46d79139bc",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "deep convolutional encoder-decoder for myelin and axon segmentation",
    "tools": ["torch"],
    "year": 2016.0,
    "id": "04327d42-c350-4022-901a-6e46d79139bc.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "microscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "mouse",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "dced",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "35",
    "training": 34.0,
    "validation": 0.0,
    "testing": 1.0,
    "cross_validation": 1.0,
    "country_1": "New Zealand",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 International Conference on Image and Vision Computing New Zealand",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "PURPOSE: To describe and evaluate a new fully automated musculoskeletal tissue segmentation method using deep convolutional neural network (CNN) and three-dimensional (3D) simplex deformable modeling to improve the accuracy and efficiency of cartilage and bone segmentation within the knee joint. METHODS: A fully automated segmentation pipeline was built by combining a semantic segmentation CNN and 3D simplex deformable modeling. A CNN technique called SegNet was applied as the core of the segmentation method to perform high resolution pixel-wise multi-class tissue classification. The 3D simplex deformable modeling refined the output from SegNet to preserve the overall shape and maintain a desirable smooth surface for musculoskeletal structure. The fully automated segmentation method was tested using a publicly available knee image data set to compare with currently used state-of-the-art segmentation methods. The fully automated method was also evaluated on two different data sets, which include morphological and quantitative MR images with different tissue contrasts. RESULTS: The proposed fully automated segmentation method provided good segmentation performance with segmentation accuracy superior to most of state-of-the-art methods in the publicly available knee image data set. The method also demonstrated versatile segmentation performance on both morphological and quantitative musculoskeletal MR images with different tissue contrasts and spatial resolutions. CONCLUSION: The study demonstrates that the combined CNN and 3D deformable modeling approach is useful for performing rapid and accurate cartilage and bone segmentation within the knee joint. The CNN has promising potential applications in musculoskeletal imaging. Magn Reson Med, 2017. \u00a9 2017 International Society for Magnetic Resonance in Medicine.",
    "authors": "Liu F, Zhou Z, Jang H, Samsonov A, Zhao G, Kijowski R.",
    "description": "Magn Reson Med.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["fliu37@wisc.edu"],
    "email_new": ["fliu37@wisc.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28733975",
    "key": "5adf95df-b744-476a-9738-34320b8a88e1",
    "keyok": 1,
    "keywords":
      "CNN; MRI; deep learning; deformable model; musculoskeletal imaging; segmentation",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "deep convolutional neural network and 3d deformable approach for tissue segmentation in musculoskeletal magnetic resonance imaging",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "5adf95df-b744-476a-9738-34320b8a88e1.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "knee",
    "location_1": "0",
    "method_1": "ced",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "www.ski10.org",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2":
      "https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/",
    "based_on_2_name": "unet",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Magnetic Resonance in Medicine",
    "impact_factor": 3.924,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The goal of this study is to evaluate the efficacy of deep convolutional neural networks (DCNNs) in differentiating subtle, intermediate, and more obvious image differences in radiography. Three different datasets were created, which included presence/absence of the endotracheal (ET) tube (n\u00a0=\u00a0300), low/normal position of the ET tube (n\u00a0=\u00a0300), and chest/abdominal radiographs (n\u00a0=\u00a0120). The datasets were split into training, validation, and test. Both untrained and pre-trained deep neural networks were employed, including AlexNet and GoogLeNet classifiers, using the Caffe framework. Data augmentation was performed for the presence/absence and low/normal ET tube datasets. Receiver operating characteristic (ROC), area under the curves (AUC), and 95% confidence intervals were calculated. Statistical differences of the AUCs were determined using a non-parametric approach. The pre-trained AlexNet and GoogLeNet classifiers had perfect accuracy (AUC 1.00) in differentiating chest vs. abdominal radiographs, using only 45 training cases. For more difficult datasets, including the presence/absence and low/normal position endotracheal tubes, more training cases, pre-trained networks, and data-augmentation approaches were helpful to increase accuracy. The best-performing network for classifying presence vs. absence of an ET tube was still very accurate with an AUC of 0.99. However, for the most difficult dataset, such as low vs. normal position of the endotracheal tube, DCNNs did not perform as well, but achieved a reasonable AUC of 0.81.",
    "authors": "Lakhani P.",
    "description": "J Digit Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["Paras.lakhani@jefferson.edu"],
    "email_new": ["Paras.lakhani@jefferson.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28600640",
    "key": "4f1ffb01-099c-4b82-b32e-3f35295de368",
    "keyok": 1,
    "keywords":
      "Artificial intelligence; Artificial neural networks (ANNs); Classification; Machine learning; Radiography",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "deep convolutional neural networks for endotracheal tube position and x-ray image classification: challenges and opportunities",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "4f1ffb01-099c-4b82-b32e-3f35295de368.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "chest",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "http://caffe.berkeleyvision.org/model_zoo.html",
    "based_on_1_name": "bvlc_caffenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Journal of Digital Imaging",
    "impact_factor": 1.407,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Recently, more and more attention is drawn to the field of medical image synthesis across modalities. Among them, the synthesis of computed tomography (CT) image from T1-weighted magnetic resonance (MR) image is of great importance, although the mapping between them is highly complex due to large gaps of appearances of the two modalities. In this work, we aim to tackle this MR-to-CT synthesis by a novel deep embedding convolutional neural network (DECNN). Specifically, we generate the feature maps from MR images, and then transform these feature maps forward through convolutional layers in the network. We can further compute a tentative CT synthesis from the midway of the flow of feature maps, and then embed this tentative CT synthesis back to the feature maps. This embedding operation results in better feature maps, which are further transformed forward in DECNN. After repeat-ing this embedding procedure for several times in the network, we can eventually synthesize a final CT image in the end of the DECNN. We have validated our proposed method on both brain and prostate datasets, by also compar-ing with the state-of-the-art methods. Experimental results suggest that our DECNN (with repeated embedding op-erations) demonstrates its superior performances, in terms of both the perceptive quality of the synthesized CT image and the run-time cost for synthesizing a CT image.",
    "authors": "Lei Xiang, Qian Wang, Dong Nie, Yu Qiao, Dinggang Shen",
    "description": "Retrieved from arXiv. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["wang.qian@sjtu.edu.cn", "dgshen@med.unc.edu"],
    "email_new": ["wang.qian@sjtu.edu.cn", "dgshen@med.unc.edu"],
    "fullUrl": "https://arxiv.org/abs/1709.02073",
    "key": "f76fa072-49e3-486e-855e-c29f25d77f1e",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1709.02073.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "deep embedding convolutional neural network for synthesizing ct image from t1-weighted mr image",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "f76fa072-49e3-486e-855e-c29f25d77f1e.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "generation",
    "task_2": "0",
    "data_type": "sct",
    "data_type_1": "mri",
    "data_type_2": "0",
    "location": "prostate",
    "location_1": "brain",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.adni-info.org/Scientists/ADNIData.html",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "Korea",
    "journal": "medical image analysis",
    "impact_factor": 4.188,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Purpose: To evaluate the efficacy of deep convolutional neural networks (DCNNs) for detecting tuberculosis (TB) on chest radiographs. Materials and Methods: Four deidentified HIPAA-compliant datasets were used in this study that were exempted from review by the institutional review board, which consisted of 1007 posteroanterior chest radiographs. The datasets were split into training (68.0%), validation (17.1%), and test (14.9%). Two different DCNNs, AlexNet and GoogLeNet, were used to classify the images as having manifestations of pulmonary TB or as healthy. Both untrained and pretrained networks on ImageNet were used, and augmentation with multiple preprocessing techniques. Ensembles were performed on the best-performing algorithms. For cases where the classifiers were in disagreement, an independent board-certified cardiothoracic radiologist blindly interpreted the images to evaluate a potential radiologist-augmented workflow. Receiver operating characteristic curves and areas under the curve (AUCs) were used to assess model performance by using the DeLong method for statistical comparison of receiver operating characteristic curves. Results: The best-performing classifier had an AUC of 0.99, which was an ensemble of the AlexNet and GoogLeNet DCNNs. The AUCs of the pretrained models were greater than that of the untrained models (P &lt; .001). Augmenting the dataset further increased accuracy (P values for AlexNet and GoogLeNet were .03 and .02, respectively). The DCNNs had disagreement in 13 of the 150 test cases, which were blindly reviewed by a cardiothoracic radiologist, who correctly interpreted all 13 cases (100%). This radiologist-augmented approach resulted in a sensitivity of 97.3% and specificity 100%. Conclusion: Deep learning with DCNNs can accurately classify TB at chest radiography with an AUC of 0.99. A radiologist-augmented approach for cases where there was disagreement among the classifiers further improved accuracy. \u00a9 RSNA, 2017.",
    "authors": "Lakhani P, Sundaram B.",
    "description": "Radiology.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["paras.lakhani@jefferson.edu"],
    "email_new": ["paras.lakhani@jefferson.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28436741",
    "key": "80262690-ca74-4789-b58f-d648f35b39b8",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks",
    "tools": ["digits", "caffe"],
    "year": 2017.0,
    "id": "80262690-ca74-4789-b58f-d648f35b39b8.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "chest",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",
    "based_on_1_name": "alexnet",
    "based_on_2":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
    "based_on_2_name": "bvlc_googlelenet",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Radiology",
    "impact_factor": 7.297,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Objective: The advent of Electronic Medical Records (EMR) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography (OCT) is the most commonly obtained imaging modality in ophthalmology and represents a dense and rich dataset when combined with labels derived from the EMR. We sought to determine if deep learning could be utilized to distinguish normal OCT images from images from patients with Age-related Macular Degeneration (AMD). Methods: Automated extraction of an OCT imaging database was performed and linked to clinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical endpoints extracted from EPIC. The central 11 images were selected from each OCT scan of two cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Area under receiver operator curves (auROC) were constructed at an independent image level, macular OCT level, and patient level. Results: Of an extraction of 2.6 million OCT images linked to clinical datapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an auROC of 92.78% with an accuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an accuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an accuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were 92.64% and 93.69% respectively. Conclusions: Deep learning techniques are effective for classifying OCT images. These findings have important implications in utilizing OCT in automated screening and computer aided diagnosis tools.",
    "authors": "Cecilia S. Lee, Doug M. Baughman, Aaron Y. Lee",
    "description": "4 Figures, 1 Table. 2016",
    "doi": null,
    "duplicate": false,
    "email": ["leeay@uw.edu"],
    "email_new": ["leeay@uw.edu"],
    "fullUrl": "https://arxiv.org/abs/1612.04891",
    "key": "c15e6d64-6572-4f5a-b857-55094921a7bd",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1612.04891.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "deep learning is effective for the classification of oct images of normal versus age-related macular degeneration",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "c15e6d64-6572-4f5a-b857-55094921a7bd.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "oct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Ophthalmology Retina",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.",
    "authors": "Li R, Zeng T, Peng H, Ji S.",
    "description": "IEEE Trans Med Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "tzeng@eecs.wsu.edu",
      "sji@@eecs.wsu.edu",
      "hanchuanp@alleninstitute.org"
    ],
    "email_new": [
      "tzeng@eecs.wsu.edu",
      "sji@@eecs.wsu.edu",
      "hanchuanp@alleninstitute.org"
    ],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28287966",
    "key": "d932ab7c-0ece-4f62-be87-204bf6f3982a",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "deep learning segmentation of optical microscopy images improves 3-d neuron reconstruction",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "d932ab7c-0ece-4f62-be87-204bf6f3982a.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "microscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "https://alleninstitute.org/bigneuron/about/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/google/inception",
    "based_on_1_name": "google_inception",
    "based_on_2": "https://github.com/KaimingHe/deep-residual-networks",
    "based_on_2_name": "deep_residual_networks",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Motivation: Progress in 3D electron microscopy (EM) imaging has greatly facilitated neuroscience research in high-throughput data acquisition. Correspondingly, high-throughput automated image analysis methods are necessary to work on par with the speed of data being produced. One such example is the need for automated EM image segmentation for neurite reconstruction. However, the efficiency and reliability of current methods are still lagging far behind human performance. Results: Here, we propose DeepEM3D, a deep learning method for segmenting 3D anisotropic brain electron microscopy images. In this method, the deep learning model can efficiently build feature representation and incorporate sufficient multi-scale contextual information. We propose employing a combination of novel boundary map generation methods with optimized model ensembles to address the inherent challenges of segmenting anisotropic images. We evaluated our method by participating in the 3D segmentation of neurites in EM images (SNEMI3D) challenge. Our submission is ranked #1 on the current leaderboard as of Oct 15, 2016. More importantly, our result was very close to human-level performance in terms of the challenge evaluation metric: namely, a Rand error of 0.06015 versus the human value of 0.05998. Availability and Implementation: The code is available at https://github.com/divelab/deepem3d/. Contact: sji@eecs.wsu.edu. Supplementary information: Supplementary data are available at Bioinformatics online.",
    "authors": "Zeng T, Wu B, Ji S.",
    "description": "Bioinformatics.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["sji@eecs.wsu.edu"],
    "email_new": ["sji@eecs.wsu.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28379412",
    "key": "a0f317d8-a596-4a3f-b867-846499ba1b97",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "deepem3d: approaching human-level performance on 3d anisotropic em image segmentation",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "a0f317d8-a596-4a3f-b867-846499ba1b97.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "em",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://brainiac2.mit.edu/SNEMI3D/user/register",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "https://github.com/divelab/deepem3d/",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/KaimingHe/deep-residual-networks",
    "based_on_1_name": "deep_residual_networks",
    "based_on_2": "https://github.com/google/inception",
    "based_on_2_name": "google_inception",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Bioinformatics",
    "impact_factor": 7.307,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into \u201dabnormal\u201d and \u201dnormal\u201d categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells \u2013 without prior segmentation \u2013 based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.",
    "authors": "L. Zhang,  L. Lu,  I. Nogues,  R. Summers,  S. Liu,  J. Yao",
    "description": "IEEE Journal of Biomedical and Health Informatics. 2017",
    "doi": "10.1109/JBHI.2017.2705583",
    "duplicate": false,
    "email": ["ling.zhang3@nih.gov", "jyao@nih.gov"],
    "email_new": ["ling.zhang3@nih.gov", "jyao@nih.gov"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932065",
    "key": "4b5193c9-e8bf-4b0a-9ce4-5cae475dd1b8",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "deeppap: deep convolutional networks for cervical cell classification",
    "tools": ["caffe", "convnet"],
    "year": 2017.0,
    "id": "4b5193c9-e8bf-4b0a-9ce4-5cae475dd1b8.pdf",
    "use": 1,
    "domain": "cytology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "pap smear",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cervix",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://mde-lab.aegean.gr/downloads",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
    "based_on_1_name": "bvlc_caffenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "China",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the diagnosis of AD and MCI using structural Magnetic Resonance Imaging (MRI) scans. In this paper, we propose the use of a Convolutional Neural Network (CNN) in the detection of AD and MCI. In particular, we modified the 16-layered VGGNet for the 3-way classification of AD, MCI and Healthy Controls (HC) on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset achieving an overall accuracy of 91.85% and outperforming several classifiers from other studies.",
    "authors":
      "C. D. Billones,  O. J. L. D. Demetria,  D. E. D. Hostallero,  P. C. Naval",
    "description": "2016 IEEE Region 10 Conference (TENCON). 2016",
    "doi": "10.1109/TENCON.2016.7848755",
    "duplicate": false,
    "email": ["pcnaval@dcs.upd.edu.ph"],
    "email_new": ["pcnaval@dcs.upd.edu.ph"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848755",
    "key": "279c6394-f97b-4307-99d8-71d593434033",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "demnet: a convolutional neural network for the detection of alzheimer's disease and mild cognitive impairment",
    "tools": ["digits", "caffe"],
    "year": 2016.0,
    "id": "279c6394-f97b-4307-99d8-71d593434033.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "adni.loni.usc.edu",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": "900",
    "training": 630.0,
    "validation": 0.0,
    "testing": 270.0,
    "cross_validation": 0.0,
    "country_1": "Philippines",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2016 IEEE Region 10 Conference (TENCON)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "PURPOSE: Colitis refers to inflammation of the inner lining of the colon that is frequently associated with infection and allergic reactions. In this paper, we propose deep convolutional neural networks methods for lesion-level colitis detection and a support vector machine (SVM) classifier for patient-level colitis diagnosis on routine abdominal CT scans. METHODS: The recently developed Faster Region-based Convolutional Neural Network (Faster RCNN) is utilized for lesion-level colitis detection. For each 2D slice, rectangular region proposals are generated by region proposal networks (RPN). Then, each region proposal is jointly classified and refined by a softmax classifier and bounding-box regressor. Two convolutional neural networks, eight layers of ZF net and 16 layers of VGG net are compared for colitis detection. Finally, for each patient, the detections on all 2D slices are collected and a SVM classifier is applied to develop a patient-level diagnosis. We trained and evaluated our method with 80 colitis patients and 80 normal cases using 4\u00a0\u00d7\u00a04-fold cross validation. RESULTS: For lesion-level colitis detection, with ZF net, the mean of average precisions (mAP) were 48.7% and 50.9% for RCNN and Faster RCNN, respectively. The detection system achieved sensitivities of 51.4% and 54.0% at two false positives per patient for RCNN and Faster RCNN, respectively. With VGG net, Faster RCNN increased the mAP to 56.9% and increased the sensitivity to 58.4% at two false positive per patient. For patient-level colitis diagnosis, with ZF net, the average areas under the ROC curve (AUC) were 0.978\u00a0\u00b1\u00a00.009 and 0.984\u00a0\u00b1\u00a00.008 for RCNN and Faster RCNN method, respectively. The difference was not statistically significant with P\u00a0=\u00a00.18. At the optimal operating point, the RCNN method correctly identified 90.4% (72.3/80) of the colitis patients and 94.0% (75.2/80) of normal cases. The sensitivity improved to 91.6% (73.3/80) and the specificity improved to 95.0% (76.0/80) for the Faster RCNN method. With VGG net, Faster RCNN increased the AUC to 0.986\u00a0\u00b1\u00a00.007 and increased the diagnosis sensitivity to 93.7% (75.0/80) and specificity was unchanged at 95.0% (76.0/80). CONCLUSION: Colitis detection and diagnosis by deep convolutional neural networks is accurate and promising for future clinical application. Published 2017. This article is a U.S. Government work and is in the public domain in the USA.",
    "authors":
      "Liu J, Wang D, Lu L, Wei Z, Kim L, Turkbey EB, Sahiner B, Petrick NA, Summers RM.",
    "description": "Med Phys.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["rms@nih.gov"],
    "email_new": ["rms@nih.gov"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28594460",
    "key": "53aaa619-cfc3-4fa7-93ad-a399933683d3",
    "keyok": 1,
    "keywords": "RPN; Region-based CNN; colitis detection; colitis diagnosis",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "detection and diagnosis of colitis on computed tomography using deep convolutional neural networks",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "53aaa619-cfc3-4fa7-93ad-a399933683d3.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "detection",
    "task_2": "diagnosis",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "colon",
    "location_1": "0",
    "method_1": "regioncnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "medical physics ",
    "impact_factor": 2.635,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.",
    "authors": "D. Sarikaya,  J. J. Corso,  K. A. Guru",
    "description": "IEEE Transactions on Medical Imaging. 2017",
    "doi": "10.1109/TMI.2017.2665671",
    "duplicate": false,
    "email": ["duy-gusar@buffalo.edu"],
    "email_new": ["duy-gusar@buffalo.edu"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847313",
    "key": "e911aadd-df2d-46d4-8a06-9b0c35ff2e5b",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "e911aadd-df2d-46d4-8a06-9b0c35ff2e5b.pdf",
    "use": 1,
    "domain": "robotics",
    "task_1": "detection",
    "task_2": "localization",
    "data_type": "video",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "rcnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 1.0,
    "data_link_1":
      "https://www.roswellpark.org/education/atlas-program/research-development/dione-dataset",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/rbgirshick/py-faster-rcnn",
    "based_on_1_name": "faster_rcnn",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "BACKGROUND AND OBJECTIVE: Diabetic retinopathy is one of the leading disabling chronic diseases and one of the leading causes of preventable blindness in developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into automated population screening programs. Detection of exudates in color fundus photographs is very important for early diagnosis of diabetic retinopathy. METHODS: We use deep convolutional neural networks for exudate detection. In order to incorporate high level anatomical knowledge about potential exudate locations, output of the convolutional neural network is combined with the output of the optic disc detection and vessel detection procedures. RESULTS: In the validation step using a manually segmented image database we obtain a maximum F1 measure of 0.78. CONCLUSIONS: As manually segmenting and counting exudate areas is a tedious task, having a reliable automated output, such as automated segmentation using convolutional neural networks in combination with other landmark detectors, is an important step in creating automated screening programs for early detection of diabetic retinopathy. Copyright \u00a9 2016 Elsevier Ireland Ltd. All rights reserved.",
    "authors": "Prenta\u0161i\u0107 P, Lon\u010dari\u0107 S.",
    "description": "Comput Methods Programs Biomed.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["pavle.prentasic@fer.hr"],
    "email_new": ["pavle.prentasic@fer.hr"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28110732",
    "key": "15de50b5-be30-42fc-b6e0-ab0f0770d7d0",
    "keyok": 1,
    "keywords":
      "Convolutional neural networks; Diabetic retinopathy; Exudates; Fundus photographs; Machine learning",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "detection of exudates in fundus photographs using deep neural networks and anatomical landmark detection fusion",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "15de50b5-be30-42fc-b6e0-ab0f0770d7d0.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "fundus photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://ieeexplore.ieee.org/document/6703830/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "50",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "Croatia",
    "country_2": "0",
    "country_3": "0",
    "journal": "Computer Methods and Programs in Biomedicine",
    "impact_factor": 2.503,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "BACKGROUND: Ongoing research into inflammatory conditions raises an increasing need to evaluate immune cells in histological sections in biologically relevant regions of interest (ROIs). Herein, we compare different approaches to automatically detect lobular structures in human normal breast tissue in digitized whole slide images (WSIs). This automation is required to perform objective and consistent quantitative studies on large data sets. METHODS: In normal breast tissue from nine healthy patients immunohistochemically stained for different markers, we evaluated and compared three different image analysis methods to automatically detect lobular structures in WSIs: (1) a bottom-up approach using the cell-based data for subsequent tissue level classification, (2) a top-down method starting with texture classification at tissue level analysis of cell densities in specific ROIs, and (3) a direct texture classification using deep learning technology. RESULTS: All three methods result in comparable overall quality allowing automated detection of lobular structures with minor advantage in sensitivity (approach 3), specificity (approach 2), or processing time (approach 1). Combining the outputs of the approaches further improved the precision. CONCLUSIONS: Different approaches of automated ROI detection are feasible and should be selected according to the individual needs of biomarker research. Additionally, detected ROIs could be used as a basis for quantification of immune infiltration in lobular structures. Copyright \u00a9 2016 Elsevier Ltd. All rights reserved.",
    "authors":
      "Apou G, Schaadt NS, Naegel B, Forestier G, Sch\u00f6nmeyer R, Feuerhake F, Wemmert C, Grote A.",
    "description": "Comput Biol Med.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["gapou@unistra.fr"],
    "email_new": ["gapou@unistra.fr"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27209271",
    "key": "48752b7a-3043-4a6f-ae1c-49f1d134b915",
    "keyok": 1,
    "keywords":
      "Convolutional neural network; Digital histopathology; Image analysis; Normal breast lobule; Whole slide image",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title": "detection of lobular structures in normal breast tissue",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "48752b7a-3043-4a6f-ae1c-49f1d134b915.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "wsi",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "9",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "Germany",
    "country_2": "France",
    "country_3": "0",
    "journal": "Computers in Biology and Medicine",
    "impact_factor": 1.836,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After manual labelling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks: a deep convolution autoencoder (DCAE) for cardiac image representation, and a multiple output convolution neural network (CNN) for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices. When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.440.71mm) and areas of cavity and myocardium (204133mm2). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment",
    "authors": "W. Xue,  A. Islam,  M. Bhaduri,  S. Li",
    "description": "IEEE Transactions on Medical Imaging. 2017",
    "doi": "10.1109/TMI.2017.2709251",
    "duplicate": false,
    "email": ["slishuo@gmail.com"],
    "email_new": ["slishuo@gmail.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934404",
    "key": "d402996b-007f-414f-b190-0658dd9a4e78",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "direct multitype cardiac indices estimation via joint representation and regression learning",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "d402996b-007f-414f-b190-0658dd9a4e78.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "regression",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "heart",
    "location_1": "0",
    "method_1": "ced",
    "method_2": "cnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/shelhamer/fcn.berkeleyvision.org",
    "based_on_1_name": "bvlc_fully_convolutional_nets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Canada",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In the demand for protecting the increasing aged groups from heart attacks, the improvement of the mobile electrocardiogram (ECG) monitoring systems becomes significant. The limitations of the arrhythmia classification in these systems are the lack of ability to cope with motion state and the low accuracy in new users' data. This paper proposes a system which applies the impulse radio ultra wideband radar data as additional information to assist the arrhythmia classification of ECG recordings in the slight motion state. Besides, this proposed system employs a cascade convolutional neural network to achieve an integrated analysis of ECG recordings and radar data. The experiments are implemented in the Caffe platform and the result reaches an accuracy of 88.89% in the slight motion state. It turns out that this proposed system keeps a stable accuracy of classification for normal and abnormal heartbeats in the slight motion state.",
    "authors": "W. Yin,  X. Yang,  L. Zhang,  E. Oki",
    "description": "IEEE Access. 2016",
    "doi": "10.1109/ACCESS.2016.2608777",
    "duplicate": false,
    "email": ["ywf2014@bupt.edu.cn"],
    "email_new": ["ywf2014@bupt.edu.cn"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576640",
    "key": "85236bc2-7b11-4acb-8da5-a554a9f09f59",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "ecg monitoring system integrated with ir-uwb radar based on cnn",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "85236bc2-7b11-4acb-8da5-a554a9f09f59.pdf",
    "use": 1,
    "domain": "electrocardiography",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "ecg",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "heart",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Access",
    "impact_factor": 3.244,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the visual features used are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Extensive experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.",
    "authors":
      "Andru P. Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy",
    "description": "Video: this https URL. 2016",
    "doi": null,
    "duplicate": false,
    "email": ["twinanda@unistra.fr"],
    "email_new": ["twinanda@unistra.fr"],
    "fullUrl": "https://arxiv.org/abs/1602.03012",
    "key": "c9b6fe1b-1c0d-4c8e-97d1-f8fd0919d4f6",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1602.03012.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "endonet: a deep architecture for recognition tasks on laparoscopic videos",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "c9b6fe1b-1c0d-4c8e-97d1-f8fd0919d4f6.pdf",
    "use": 1,
    "domain": "laparoscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "video",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gallbladder",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 1.0,
    "data_link_1": "https://endovis.grand-challenge.org/",
    "data_link_2": "http://camma.u-strasbg.fr/datasets",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "France",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our work assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.",
    "authors": "Huang Y, Zheng H, Liu C, Ding X, Rohde G.",
    "description": "IEEE J Biomed Health Inform.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["yhuang2010@xmu.edu.cn", "dxh@xmu.edu.cn"],
    "email_new": ["yhuang2010@xmu.edu.cn", "dxh@xmu.edu.cn"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28410112",
    "key": "361c7ffa-8455-4deb-a7cf-37fc6604adcb",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "epithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "361c7ffa-8455-4deb-a7cf-37fc6604adcb.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "histopathology",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "https://github.com/KaimingHe/deep-residual-networks",
    "based_on_1_name": "deep_residual_networks",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": "1219",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.",
    "authors":
      "P. Stanitsas,  A. Cherian,  Xinyan Li,  A. Truskinovsky,  V. Morellas,  N. Papanikolopoulos",
    "description":
      "2016 23rd International Conference on Pattern Recognition (ICPR). 2016",
    "doi": "10.1109/ICPR.2016.7899848",
    "duplicate": false,
    "email": ["papan001@umn.edu"],
    "email_new": ["papan001@umn.edu"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899848",
    "key": "ec322014-095c-497c-8199-a06698616b61",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "evaluation of feature descriptors for cancerous tissue recognition",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "ec322014-095c-497c-8199-a06698616b61.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "histapathology",
    "data_type_1": "H&E stained",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "prostate",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "Australia",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 23rd International Conference on Pattern Recognition (ICPR)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "We introduce a deep encoder-decoder architecture for image deformation prediction from multimodal images. Specifically, we design an image-patch-based deep network that jointly (i) learns an image similarity measure and (ii) the relationship between image patches and deformation parameters. While our method can be applied to general image registration formulations, we focus on the Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By predicting the initial momentum of the shooting formulation of LDDMM, we preserve its mathematical properties and drastically reduce the computation time, compared to optimization-based approaches. Furthermore, we create a Bayesian probabilistic version of the network that allows evaluation of registration uncertainty via sampling of the network at test time. We evaluate our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our experiments show that our method generates accurate predictions and that learning the similarity measure leads to more consistent registrations than relying on generic multimodal image similarity measures, such as mutual information. Our approach is an order of magnitude faster than optimization-based LDDMM.",
    "authors": "Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer",
    "description": "Accepted as a conference paper for ISBI 2017. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["xy@cs.unc.edu"],
    "email_new": ["xy@cs.unc.edu"],
    "fullUrl": "https://arxiv.org/abs/1703.10902",
    "key": "b9e7fc20-e2ef-49ed-aff7-d95daaa47d74",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1703.10902.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title": "fast predictive multimodal image registration",
    "tools": ["torch"],
    "year": 2017.0,
    "id": "b9e7fc20-e2ef-49ed-aff7-d95daaa47d74.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "registration",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "ced",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.ibis-network.org/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "Austria",
    "country_3": "0",
    "journal": "NeuroImage",
    "impact_factor": 5.835,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The quality of ultrasound (US) images for the obstetric examination is crucial for accurate biometric measurement. However, manual quality control is a labor intensive process and often impractical in a clinical setting. To improve the efficiency of examination and alleviate the measurement error caused by improper US scanning operation and slice selection, a computerized fetal US image quality assessment (FUIQA) scheme is proposed to assist the implementation of US image quality control in the clinical obstetric examination. The proposed FUIQA is realized with two deep convolutional neural network models, which are denoted as L-CNN and C-CNN, respectively. The L-CNN aims to find the region of interest (ROI) of the fetal abdominal region in the US image. Based on the ROI found by the L-CNN, the C-CNN evaluates the image quality by assessing the goodness of depiction for the key structures of stomach bubble and umbilical vein. To further boost the performance of the L-CNN, we augment the input sources of the neural network with the local phase features along with the original US data. It will be shown that the heterogeneous input sources will help to improve the performance of the L-CNN. The performance of the proposed FUIQA is compared with the subjective image quality evaluation results from three medical doctors. With comprehensive experiments, it will be illustrated that the computerized assessment with our FUIQA scheme can be comparable to the subjective ratings from medical doctors.",
    "authors": "L. Wu,  J. Z. Cheng,  S. Li,  B. Lei,  T. Wang,  D. Ni",
    "description": "IEEE Transactions on Cybernetics. 2017",
    "doi": "10.1109/TCYB.2017.2671898",
    "duplicate": false,
    "email": ["nidong@szu.edu.cn"],
    "email_new": ["nidong@szu.edu.cn"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875138",
    "key": "9eb23897-e5bb-4d28-931e-ff3cceff8a15",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "fuiqa: fetal ultrasound image quality assessment with deep convolutional networks",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "9eb23897-e5bb-4d28-931e-ff3cceff8a15.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "quality assesment",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "fetal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Cybernetics",
    "impact_factor": 7.384,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The interpretation and analysis of wireless capsule endoscopy (WCE) recordings is a complex task which requires sophisticated computer aided decision (CAD) systems to help physicians with video screening and, finally, with the diagnosis. Most CAD systems used in capsule endoscopy share a common system design, but use very different image and video representations. As a result, each time a new clinical application of WCE appears, a new CAD system has to be designed from the scratch. This makes the design of new CAD systems very time consuming. Therefore, in this paper we introduce a system for small intestine motility characterization, based on Deep Convolutional Neural Networks, which circumvents the laborious step of designing specific features for individual motility events. Experimental results show the superiority of the learned features over alternative classifiers constructed using state-of-the-art handcrafted features. In particular, it reaches a mean classification accuracy of 96% for six intestinal motility events, outperforming the other classifiers by a large margin (a 14% relative performance increase). Copyright \u00a9 2016 Elsevier Ltd. All rights reserved.",
    "authors":
      "Segu\u00ed S, Drozdzal M, Pascual G, Radeva P, Malagelada C, Azpiroz F, Vitri\u00e0 J.",
    "description": "Comput Biol Med.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["santi.segui@ub.edu"],
    "email_new": ["santi.segui@ub.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27810622",
    "key": "00de577b-26a3-47ce-9a98-27259e3c3c3b",
    "keyok": 1,
    "keywords":
      "Deep learning; Feature learning; Motility analysis; Wireless capsule endoscopy",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title": "generic feature learning for wireless capsule endoscopy analysis",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "00de577b-26a3-47ce-9a98-27259e3c3c3b.pdf",
    "use": 1,
    "domain": "endoscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "wireless capsule endoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gastrointestinal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "120000",
    "training": 100000.0,
    "validation": 0.0,
    "testing": 20000.0,
    "cross_validation": 0.0,
    "country_1": "Spain",
    "country_2": "Israel",
    "country_3": "0",
    "journal": "Computers in Biology and Medicine",
    "impact_factor": 1.836,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In computerized detection of clustered microcalcifications (MCs) from mammograms, the traditional approach is to apply a pattern detector to locate the presence of individual MCs, which are subsequently grouped into clusters. Such an approach is often susceptible to the occurrence of false positives (FPs) caused by local image patterns that resemble MCs. We investigate the feasibility of a direct detection approach to determining whether an image region contains clustered MCs or not. Toward this goal, we develop a deep convolutional neural network (CNN) as the classifier model to which the input consists of a large image window ([Formula: see text] in size). The multiple layers in the CNN classifier are trained to automatically extract image features relevant to MCs at different spatial scales. In the experiments, we demonstrated this approach on a dataset consisting of both screen-film mammograms and full-field digital mammograms. We evaluated the detection performance both on classifying image regions of clustered MCs using a receiver operating characteristic (ROC) analysis and on detecting clustered MCs from full mammograms by a free-response receiver operating characteristic analysis. For comparison, we also considered a recently developed MC detector with FP suppression. In classifying image regions of clustered MCs, the CNN classifier achieved 0.971 in the area under the ROC curve, compared to 0.944 for the MC detector. In detecting clustered MCs from full mammograms, at 90% sensitivity, the CNN classifier obtained an FP rate of 0.69\u00a0clusters/image, compared to 1.17\u00a0clusters/image by the MC detector. These results indicate that using global image features can be more effective in discriminating clustered MCs from FPs caused by various sources, such as linear structures, thereby providing a more accurate detection of clustered MCs on mammograms.",
    "authors": "Wang J, Nishikawa RM, Yang Y.",
    "description": "J Med Imaging (Bellingham).  2017",
    "doi": null,
    "duplicate": false,
    "email": ["yy@ece.iit.edu"],
    "email_new": ["yy@ece.iit.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28466029",
    "key": "95e648c8-3712-43c4-ab77-fda549bb6e49",
    "keyok": 1,
    "keywords":
      "clustered microcalcifications; computer-aided detection; convolutional neural network; deep learning",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "global detection approach for clustered microcalcifications in mammograms using a deep learning network",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "95e648c8-3712-43c4-ab77-fda549bb6e49.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "mammogram",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Journal of medical imaging",
    "impact_factor": 1.109,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "A quantitative model to genetically interpret the histology in whole microscopy slide images is desirable to guide downstream immunohistochemistry, genomics, and precision medicine. We constructed a statistical model that predicts whether or not SPOP is mutated in prostate cancer, given only the digital whole slide after standard hematoxylin and eosin [H&E] staining. Using a TCGA cohort of 177 prostate cancer patients where 20 had mutant SPOP, we trained multiple ensembles of residual networks, accurately distinguishing SPOP mutant from SPOP non-mutant patients (test AUROC=0.74, p=0.0007 Fisher's Exact Test).  We further validated our full metaensemble classifier on an independent test cohort from MSK-IMPACT of 152 patients where 19 had mutant SPOP.  Mutants and non-mutants were accurately distinguished despite TCGA slides being frozen sections and MSK-IMPACT slides being formalin-fixed paraffin-embedded sections (AUROC=0.86, p=0.0038). Moreover, we scanned an additional 36 MSK-IMPACT patient having mutant SPOP, trained on this expanded MSK-IMPACT cohort (test AUROC=0.75, p=0.0002), tested on the TCGA cohort (AUROC=0.64, p=0.0306), and again accurately distinguished mutants from non-mutants using the same pipeline. Importantly, our method demonstrates tractable deep learning in this small data setting of 20-55 positive examples and quantifies each prediction's uncertainty with confidence intervals. To our knowledge, this is the first statistical model to predict a genetic mutation in cancer directly from the patient's digitized H&E-stained whole microscopy slide.  Moreover, this is the first time quantitative features learned from patient genetics and histology have been used for content-based image retrieval, finding similar patients for a given patient where the histology appears to share the same genetic driver of disease i.e. SPOP mutation (p=0.0241 Kost's Method), and finding similar patients for a given patient that does not have have that driver mutation (p=0.0170 Kost's Method).",
    "authors": "Andrew J Schaumberg, Mark A Rubin, Thomas J Fuchs",
    "description": "Retrieved from bioRxiv. 2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "ajs625@cornell.edu",
      "rubinma@med.cornell.edu",
      "fuchst@mskcc.org"
    ],
    "email_new": [
      "ajs625@cornell.edu",
      "rubinma@med.cornell.edu",
      "fuchst@mskcc.org"
    ],
    "fullUrl": "http://www.biorxiv.org/content/early/2017/05/12/064279.1",
    "key": "f7431639-d6cc-48f2-8772-8fdb4fd6f2e7",
    "keyok": 1,
    "keywords": null,
    "pdfUrl":
      "http://www.biorxiv.org/content/early/2017/05/12/064279.1.full.pdf",
    "phase": 1,
    "received": 0,
    "source": "biorxiv",
    "title":
      "h&e-stained whole slide image deep learning predicts spop mutation state in prostate cancer",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "f7431639-d6cc-48f2-8772-8fdb4fd6f2e7.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "wsi",
    "data_type_1": "H&E stained",
    "data_type_2": "0",
    "location": "prostate",
    "location_1": "0",
    "method_1": "rcnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.cbioportal.org/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/KaimingHe/deep-residual-networks",
    "based_on_1_name": "deep_residual_networks",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Purpose: An end-to-end deep convolutional neural network (CNN) based on deep residual network (ResNet) was proposed to efficiently reconstruct reliable T2 mapping from single-shot OverLapping-Echo Detachment (OLED) planar imaging. Methods: The training dataset was obtained from simulations carried out on SPROM software developed by our group. The relationship between the original OLED image containing two echo signals and the corresponded T2 mapping was learned by ResNet training. After the ResNet was trained, it was applied to reconstruct the T2 mapping from simulation and in vivo human brain data. Results: Though the ResNet was trained entirely on simulated data, the trained network was generalized well to real human brain data. The results from simulation and in vivo human brain experiments show that the proposed method significantly outperformed the echo-detachment-based method. Reliable T2 mapping was achieved within tens of milliseconds after the network had been trained while the echo-detachment-based OLED reconstruction method took minutes. Conclusion: The proposed method will greatly facilitate real-time dynamic and quantitative MR imaging via OLED sequence, and ResNet has the potential to reconstruct images from complex MRI sequence efficiently.",
    "authors":
      "Congbo Cai, Yiqing Zeng, Chao Wang, Shuhui Cai, Jun Zhang, Zhong Chen, Xinghao Ding, Jianhui Zhong",
    "description": "18 pages, 7 figures. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["dxh@xmu.edu.cn"],
    "email_new": ["dxh@xmu.edu.cn"],
    "fullUrl": "https://arxiv.org/abs/1708.05170",
    "key": "7f0e1c55-f20f-4681-9532-d006f8bc0210",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1708.05170.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "high efficient reconstruction of single-shot t2 mapping from overlapping-echo detachment planar imaging based on deep residual network",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "7f0e1c55-f20f-4681-9532-d006f8bc0210.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "reconstruction",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/KaimingHe/deep-residual-networks",
    "based_on_1_name": "deep_residual_networks",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automated cell classification in Indirect Immunofluorescence (IIF) images has potential to be an important tool in clinical practice and research. This paper presents a framework for classification of Human Epithelial Type 2 cell IIF images using convolutional neural networks (CNNs). Previuos state-of-the-art methods show classification accuracy of 75.6% on a benchmark dataset. We conduct an exploration of different strategies for enhancing, augmenting and processing training data in a CNN framework for image classification. Our proposed strategy for training data and pre-training and fine-tuning the CNN network led to a significant increase in the performance over other approaches that have been used until now. Specifically, our method achieves a 80.25% classification accuracy. Source code and models to reproduce the experiments in the paper is made publicly available.",
    "authors": "N. Bayramoglu,  J. Kannala,  J. Heikkil\u00e4",
    "description":
      "2015 IEEE 15th International Conference on Bioinformatics and Bioengineering (BIBE). 2015",
    "doi": "10.1109/BIBE.2015.7367705",
    "duplicate": false,
    "email": ["jkannala@ee.oulu.fi", "jth@ee.oulu.fi", "nyalcinb@ee.oulu.fi"],
    "email_new": [
      "jkannala@ee.oulu.fi",
      "jth@ee.oulu.fi",
      "nyalcinb@ee.oulu.fi"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367705",
    "key": "962c98be-05c9-487b-a75b-ce20c74f5a6c",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "human epithelial type 2 cell classification with convolutional neural networks",
    "tools": ["convnet", "caffe"],
    "year": 2015.0,
    "id": "962c98be-05c9-487b-a75b-ce20c74f5a6c.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "iif",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://staff.itee.uq.edu.au/lovell/snphep2/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "http://www.ee.oulu.fi/~nyalcinb/papers/bibe2015/",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://code.google.com/archive/p/cuda-convnet/",
    "based_on_1_name": "cuda_convnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Finland",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2015 IEEE 15th International Conference on Bioinformatics and Bioengineering (BIBE)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Limited angle problem is a challenging issue in x-ray computed tomography (CT) field. Iterative reconstruction methods that utilize the additional prior can suppress artifacts and improve image quality, but unfortunately require increased computation time. An interesting way is to restrain the artifacts in the images reconstructed from the practical filtered back projection (FBP) method. Frikel and Quinto have proved that the streak artifacts in FBP results could be characterized. It indicates that the artifacts created by FBP method have specific and similar characteristics in a stationary limited-angle scanning configuration. Based on this understanding, this work aims at developing a method to extract and suppress specific artifacts of FBP reconstructions for limited-angle tomography. A data-driven learning-based method is proposed based on a deep convolutional neural network. An end-to-end mapping between the FBP and artifact-free images is learned and the implicit features involving artifacts will be extracted and suppressed via nonlinear mapping. The qualitative and quantitative evaluations of experimental results indicate that the proposed method show a stable and prospective performance on artifacts reduction and detail recovery for limited angle tomography. The presented strategy provides a simple and efficient approach for improving image quality of the reconstruction results from limited projection data.",
    "authors":
      "Hanming Zhang, Liang Li, Kai Qiao, Linyuan Wang, Bin Yan, Lei Li, Guoen Hu",
    "description": "Retrieved from arXiv. 2016",
    "doi": null,
    "duplicate": false,
    "email": [
      "liliang@tsinghua.edu.cn",
      "ybspace@hotmail.com",
      "z.hanming@hotmail.com",
      "15517181502@163.com",
      "wanglinyuanwly@163.com",
      "leehotline@aliyun.com",
      "13838265028@126.com"
    ],
    "email_new": [
      "liliang@tsinghua.edu.cn",
      "ybspace@hotmail.com",
      "z.hanming@hotmail.com",
      "15517181502@163.com",
      "wanglinyuanwly@163.com",
      "leehotline@aliyun.com",
      "13838265028@126.com"
    ],
    "fullUrl": "https://arxiv.org/abs/1607.08707",
    "key": "9b61d1b4-2973-4e5f-ae96-04170ad3afeb",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1607.08707.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "image prediction for limited-angle tomography via deep learning with convolutional neural network",
    "tools": ["caffe", "convnet"],
    "year": 2016.0,
    "id": "9b61d1b4-2973-4e5f-ae96-04170ad3afeb.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "denoising",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://gdcm.sourceforge.net/wiki/index.php/Sample_DataSet",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In this paper we introduce a method to handle the challenges posed by image registration for placenta reconstruction from fetoscopic video as used in the treatment of Twinto-Twin Transfusion Syndrome (TTTS). Panorama reconstruction of the placenta greatly supports the surgeon in obtaining a complete view of the placenta to localize vascular anastomoses. The found shunts can subsequently be blocked by coagulation in the correct order. By using similarity learning in training a Convolutional Neural Network we created a novel feature extraction method, allowing robust matching of keypoints for image registration and therefore taking the most critical step in placenta reconstruction from fetoscopic video. The fetoscopic video we used for our experiments was acquired from a training simulator for TTTS surgery. We compared our method with state-of-the-art methods. The matching performance of our method is up to three times better while the mean projection error is reduced with 64% for the registered images. Our image registration method provides the ground work for a complete panorama reconstruction of the placenta.",
    "authors": "F. Gaisser,  P. P. Jonker,  T. Chiba",
    "description":
      "2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2016",
    "doi": "10.1109/CVPRW.2016.66",
    "duplicate": false,
    "email": [
      "f.gaisser@tudelft.nl",
      "p.p.jonker@tudelft.nl",
      "chiba-t@sea.plala.or.jp"
    ],
    "email_new": [
      "f.gaisser@tudelft.nl",
      "p.p.jonker@tudelft.nl",
      "chiba-t@sea.plala.or.jp"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789556",
    "key": "aaefc655-ead3-4b78-917f-56eacc5d53b2",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "image registration for placenta reconstruction",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "aaefc655-ead3-4b78-917f-56eacc5d53b2.pdf",
    "use": 1,
    "domain": "endoscopy",
    "task_1": "registration",
    "task_2": "0",
    "data_type": "fetoscopy video",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "fetal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Netherlands",
    "country_2": "Japan",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Peru\u0301, a densely occupied urban community and high-burden TB.",
    "authors":
      "Y. Cao,  C. Liu,  B. Liu,  M. J. Brunette,  N. Zhang,  T. Sun,  P. Zhang,  J. Peinado,  E. S. Garavito,  L. L. Garcia,  W. H. Curioso",
    "description":
      "2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). 2016",
    "doi": "10.1109/CHASE.2016.18",
    "duplicate": false,
    "email": ["ycao@cs.uml.edu"],
    "email_new": ["ycao@cs.uml.edu"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545842",
    "key": "adf82185-9dc4-4028-8250-0cbf71aae893",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "improving tuberculosis diagnostics using deep learning and mobile health technologies among resource-poor and marginalized communities",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "adf82185-9dc4-4028-8250-0cbf71aae893.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "chest",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.imageclef.org/2011/medical",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
    "based_on_1_name": "bvlc_googlelenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2015 IEEE First Conference on Connected Health",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.",
    "authors": "Lequan Yu, Hao Chen, Qi Dou, Jing Qin, Pheng Ann Heng.",
    "description": "IEEE J Biomed Health Inform.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["lqyu@cse.cuhk.edu.hk", "hchen@cse.cuhk.edu.hk"],
    "email_new": ["lqyu@cse.cuhk.edu.hk", "hchen@cse.cuhk.edu.hk"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28114049",
    "key": "9b21f857-fa06-4a8b-8a33-f60e62a58ccb",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "integrating online and offline three-dimensional deep learning for automated polyp detection in colonoscopy videos",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "9b21f857-fa06-4a8b-8a33-f60e62a58ccb.pdf",
    "use": 1,
    "domain": "colonoscopy",
    "task_1": "detection",
    "task_2": "classification",
    "data_type": "colonoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "colon",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://grand-challenge.org/site/polyp/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/shelhamer/fcn.berkeleyvision.org",
    "based_on_1_name": "bvlc_fully_convolutional_nets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Hong Kong",
    "country_2": "China",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's picture archiving and communication system. Instead of using full 3D medical volumes, we focus on a collection of representative ~216K 2D key images/slices (selected by clinicians for diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net language models) on document- and sentence-level texts to generate semantic labels and supervised learning via deep convolutional neural networks (CNNs) to map from images to label spaces. Disease-related key words can be predicted for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning \u201cdata-hungry\u201d obstacle in the medical domain.",
    "authors": "H. C. Shin,  Le Lu,  L. Kim,  A. Seff,  J. Yao,  R. M. Summers",
    "description":
      "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015",
    "doi": "10.1109/CVPR.2015.7298712",
    "duplicate": false,
    "email": [
      "jyao@cc.nih.gov",
      "hoochang.shin@nih.gov",
      "le.lu@nih.gov",
      "lauren.kim2@nih.gov",
      "ari.seff@nih.gov",
      "rms@nih.gov"
    ],
    "email_new": [
      "jyao@cc.nih.gov",
      "hoochang.shin@nih.gov",
      "le.lu@nih.gov",
      "lauren.kim2@nih.gov",
      "ari.seff@nih.gov",
      "rms@nih.gov"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298712",
    "key": "8d4f319c-c2e0-4db3-b7df-43656d444712",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "interleaved text/image deep mining on a large-scale radiology database",
    "tools": ["digits", "caffe"],
    "year": 2015.0,
    "id": "8d4f319c-c2e0-4db3-b7df-43656d444712.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "mining",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "mri",
    "data_type_2": "text",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "rnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "https://github.com/rsummers11/CADLab",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",
    "based_on_1_name": "alexnet",
    "based_on_2": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_2_name": "vgg_very_deep_convnets",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.",
    "authors": "R. Zhu,  R. Zhang,  D. Xue",
    "description":
      "2015 8th International Congress on Image and Signal Processing (CISP). 2015",
    "doi": "10.1109/CISP.2015.7407907",
    "duplicate": false,
    "email": [
      "zrs1991@mail.ustc.edu.cn",
      "zrong@ustc.edu.cn",
      "xuedixiu@mail.ustc.edu.cn"
    ],
    "email_new": [
      "zrs1991@mail.ustc.edu.cn",
      "zrong@ustc.edu.cn",
      "xuedixiu@mail.ustc.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407907",
    "key": "edf6b323-ae15-496e-995b-2dc0e98e107e",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "lesion detection of endoscopy images based on convolutional neural network features",
    "tools": ["digits", "caffe"],
    "year": 2015.0,
    "id": "edf6b323-ae15-496e-995b-2dc0e98e107e.pdf",
    "use": 1,
    "domain": "endoscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "endoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gastrointestinal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt",
    "based_on_1_name": "bvlc_lenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2015 8th International Congress on Image and Signal Processing (CISP 2015)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Diagnostic ultrasound offers great improvements in diagnostic accuracy and robustness. However, it is difficult to make subjective and uniform diagnoses, because the quality of ultrasound images can be easily influenced by machine settings, the characteristics of ultrasonic waves, the interactions between ultrasound and body tissues, and other uncontrollable factors. In this paper, we propose a novel liver fibrosis classification method based on transfer learning (TL) using VGGNet and a deep classifier called fully connected network (FCNet). In case of insufficient samples, deep features extracted using TL strategy can provide sufficient classification information. These deep features are then sent to FCNet for the classification of different liver fibrosis statuses. With this framework, tests show that our deep features combined with the FCNet can provide suitable information to enable the construction of the most accurate prediction model when compared with other methods.",
    "authors": "D. Meng,  L. Zhang,  G. Cao,  W. Cao,  G. Zhang,  B. Hu",
    "description": "IEEE Access. 2017",
    "doi": "10.1109/ACCESS.2017.2689058",
    "duplicate": false,
    "email": [
      "mengdan90@163.com",
      "gtcao@sei.ecnu.edu.cn",
      "zsmj@hotmail.com",
      "wmcao@szu.edu.cn",
      "gxzhang@sei.ecnu.edu.cn",
      "binghu stephen@163.com"
    ],
    "email_new": [
      "mengdan90@163.com",
      "gtcao@sei.ecnu.edu.cn",
      "zsmj@hotmail.com",
      "wmcao@szu.edu.cn",
      "gxzhang@sei.ecnu.edu.cn",
      "binghu stephen@163.com"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890483",
    "key": "7aa21208-ba83-47cf-a2ab-a25ed3427715",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "liver fibrosis classification based on transfer learning and fcnet for ultrasound images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "7aa21208-ba83-47cf-a2ab-a25ed3427715.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "liver",
    "location_1": "0",
    "method_1": "fullyconnectednn",
    "method_2": "cnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Access",
    "impact_factor": 3.244,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Given the potential X-ray radiation risk to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. The current main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction, but they need to access original raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, the deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation and lesion detection.",
    "authors":
      "Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin, Yang Chen, Peixi Liao, Jiliu Zhou, Ge Wang",
    "description": "Accepted by IEEE TMI. 2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "linfeng@scu.edu.cn",
      "huchen@scu.edu.cn",
      "zhoujl@scu.edu.cn",
      "yzhang@scu.edu.cn",
      "mkalra@mgh.harvard.edu",
      "chenyang.list@seu.edu.cn",
      "universe6527@163.com",
      "wangg6@rpi.edu"
    ],
    "email_new": [
      "linfeng@scu.edu.cn",
      "huchen@scu.edu.cn",
      "zhoujl@scu.edu.cn",
      "yzhang@scu.edu.cn",
      "mkalra@mgh.harvard.edu",
      "chenyang.list@seu.edu.cn",
      "universe6527@163.com",
      "wangg6@rpi.edu"
    ],
    "fullUrl": "https://arxiv.org/abs/1702.00288",
    "key": "35f064f0-3981-4161-89c8-d1be8dff46bd",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1702.00288.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "low-dose ct with a residual encoder-decoder convolutional neural network (red-cnn)",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "35f064f0-3981-4161-89c8-d1be8dff46bd.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "denoising",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "0",
    "location_1": "0",
    "method_1": "redcnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://imaging.nci.nih.gov/ncia/login.jsf",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "175",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Early detection of lung nodules in thoracic Computed Tomography (CT) scans is of great importance for the successful diagnosis and treatment of lung cancer. Due to improvements in screening technologies, and an increased demand for their use, radiologists are required to analyze an ever increasing amount of image data, which can affect the quality of their diagnoses. Computer-Aided Detection (CADe) systems are designed to assist radiologists in this endeavor. Here, we present a CADe system for the detection of lung nodules in thoracic CT images. Our system is based on (1) the publicly available Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) database, which contains 1018 thoracic CT scans with nodules of different shape and size, and (2) a deep Convolutional Neural Network (CNN), which is trained, using the back-propagation algorithm, to extract valuable volumetric features from the input data and detect lung nodules in sub-volumes of CT images. Considering only those test nodules that have been annotated by four radiologists, our CADe system achieves a sensitivity (true positive rate) of 78.9% with 20 false positives (FPs) per scan, or a sensitivity of 71.2% with 10 FPs per scan. This is achieved without using any segmentation or additional FP reduction procedures, both of which are commonly used in other CADe systems. Furthermore, our CADe system is validated on a larger number of lung nodules compared to other studies, which increases the variation in their appearance, and therefore, makes their detection by a CADe system more challenging.",
    "authors": "R. Golan,  C. Jacob,  J. Denzinger",
    "description":
      "2016 International Joint Conference on Neural Networks (IJCNN). 2016",
    "doi": "10.1109/IJCNN.2016.7727205",
    "duplicate": false,
    "email": [
      "grotem@ucalgary.ca",
      "cjacob@ucalgary.ca",
      "denzinge@cpsc.ucalgary.ca"
    ],
    "email_new": [
      "grotem@ucalgary.ca",
      "cjacob@ucalgary.ca",
      "denzinge@cpsc.ucalgary.ca"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727205",
    "key": "94ba9ab3-dab6-400f-995d-7fdd12a1fde8",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "lung nodule detection in ct images using deep convolutional neural networks",
    "tools": ["torch"],
    "year": 2016.0,
    "id": "94ba9ab3-dab6-400f-995d-7fdd12a1fde8.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "lung",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1":
      "https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Canada",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 International Joint Conference on Neural Networks (IJCNN)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Recent advancement in genomics technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC & its abnormalities detection and WBC respectively.",
    "authors": "M. I. Razzak,  S. Naz",
    "description":
      "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2017",
    "doi": "10.1109/CVPRW.2017.111",
    "duplicate": false,
    "email": ["mirpak@gmail.com", "saeedanaz292@gmail.com"],
    "email_new": ["mirpak@gmail.com", "saeedanaz292@gmail.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014845",
    "key": "e4a574b5-37e7-41ce-9f49-8d6bd08437f5",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "microscopic blood smear segmentation and classification using deep contour aware cnn and extreme machine learning",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "e4a574b5-37e7-41ce-9f49-8d6bd08437f5.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "segmentation",
    "task_2": "classification",
    "data_type": "blood film",
    "data_type_1": "microscopy",
    "data_type_2": "0",
    "location": "blood",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://homes.di.unimi.it/scotti/all/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Saudi Arabia",
    "country_2": "Pakistan",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 IEEE Conference on Computer Vision and Pattern Recognition",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "This paper focuses on the problem of feature extraction and the classification task of microvascular morphological type to aid esophageal cancer detection. A specialized convolutional neural network (CNN) is designed to extract hierarchical features and Support Vector Machines (SVMs) are introduced to enhance the generalization ability of classifiers. Experiments are conducted on the NBI-ME dataset, achieving a recognition rate of 88.19% on patch level. The results show that the CNN-SVM model beats models of traditional features with SVM as well as the original CNN with softmax. The synthesis results indicate this system is able to assist clinical diagnose to a certain extent.",
    "authors": "D. X. Xue,  R. Zhang,  R. S. Zhu",
    "description":
      "2015 International Symposium on Bioelectronics and Bioinformatics (ISBB). 2015",
    "doi": "10.1109/ISBB.2015.7344925",
    "duplicate": false,
    "email": [
      "xuedixiu@mail.ustc.edu.cn",
      "zrong@ustc.edu.cn",
      "zrs1991@mail.ustc.edu.cn"
    ],
    "email_new": [
      "xuedixiu@mail.ustc.edu.cn",
      "zrong@ustc.edu.cn",
      "zrs1991@mail.ustc.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344925",
    "key": "a0361dee-a14c-4d09-a259-295243474746",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "microvascular morphological type recognition using trainable feature extractor",
    "tools": ["digits", "caffe"],
    "year": 2015.0,
    "id": "a0361dee-a14c-4d09-a259-295243474746.pdf",
    "use": 1,
    "domain": "endoscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "magnification endoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "esophagus",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
    "based_on_2_name": "bvlc_googlelenet",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2015 International Symposium on Bioelectronics and Bioinformatics (ISBB)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "PURPOSE: Interests have been rapidly growing in the field of radiotherapy to replace CT with magnetic resonance imaging (MRI), due to superior soft tissue contrast offered by MRI and the desire to reduce unnecessary radiation dose. MR-only radiotherapy also simplifies clinical workflow and avoids uncertainties in aligning MR with CT. Methods, however, are needed to derive CT-equivalent representations, often known as synthetic CT (sCT), from patient MR images for dose calculation and DRR-based patient positioning. Synthetic CT estimation is also important for PET attenuation correction in hybrid PET-MR systems. We propose in this work a novel deep convolutional neural network (DCNN) method for sCT generation and evaluate its performance on a set of brain tumor patient images. METHODS: The proposed method builds upon recent developments of deep learning and convolutional neural networks in the computer vision literature. The proposed DCNN model has 27 convolutional layers interleaved with pooling and unpooling layers and 35 million free parameters, which can be trained to learn a direct end-to-end mapping from MR images to their corresponding CTs. Training such a large model on our limited data is made possible through the principle of transfer learning and by initializing model weights from a pretrained model. Eighteen brain tumor patients with both CT and T1-weighted MR images are used as experimental data and a sixfold cross-validation study is performed. Each sCT generated is compared against the real CT image of the same patient on a voxel-by-voxel basis. Comparison is also made with respect to an atlas-based approach that involves deformable atlas registration and patch-based atlas fusion. RESULTS: The proposed DCNN method produced a mean absolute error (MAE) below 85 HU for 13 of the 18 test subjects. The overall average MAE was 84.8 \u00b1 17.3 HU for all subjects, which was found to be significantly better than the average MAE of 94.5 \u00b1 17.8 HU for the atlas-based method. The DCNN method also provided significantly better accuracy when being evaluated using two other metrics: the mean squared error (188.6 \u00b1 33.7 versus 198.3 \u00b1 33.0) and the Pearson correlation coefficient(0.906 \u00b1 0.03 versus 0.896 \u00b1 0.03). Although training a DCNN model can be slow, training only need be done once. Applying a trained model to generate a complete sCT volume for each new patient MR image only took 9 s, which was much faster than the atlas-based approach. CONCLUSIONS: A DCNN model method was developed, and shown to be able to produce highly accurate sCT estimations from conventional, single-sequence MR images in near real time. Quantitative results also showed that the proposed method competed favorably with an atlas-based method, in terms of both accuracy and computation speed at test time. Further validation on dose computation accuracy and on a larger patient cohort is warranted. Extensions of the method are also possible to further improve accuracy or to handle multi-sequence MR images. \u00a9 2017 American Association of Physicists in Medicine.",
    "authors": "Han X.",
    "description": "Med Phys.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["xiao.han@elekta.com"],
    "email_new": ["xiao.han@elekta.com"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28192624",
    "key": "e1096c87-1ba3-42bd-9c50-f87ffa2fcf43",
    "keyok": 1,
    "keywords":
      "MRI; convolutional neural network; deep learning; radiation therapy; synthetic CT",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "mr-based synthetic ct generation using a deep convolutional neural network method",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "e1096c87-1ba3-42bd-9c50-f87ffa2fcf43.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "generation",
    "task_2": "0",
    "data_type": "sct",
    "data_type_1": "mri",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "cnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2":
      "https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/",
    "based_on_2_name": "unet",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "medical physics",
    "impact_factor": 2.635,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "With the development of various imaging technologies, medical imaging has been playing more important roles on providing scientific proof for doctors to make decisions on clinical diagnosis. At the same time, it is very important to excavate valuable information hidden in those images and take over some auxiliary medical works from doctors by the computer. Therefore, a large number of image segmentation methods, including some classic algorithms have been proposed and some of them perform well. In this paper, we built a deep convolutional neural network to segment the MRI brain images. Results show that the network has a good performance on segmentation of the gray and white matter of brains, it also had a good generalization ability.",
    "authors": "Y. Wang,  Z. Sun,  C. Liu,  W. Peng,  J. Zhang",
    "description":
      "2016 IEEE International Conference on Mechatronics and Automation. 2016",
    "doi": "10.1109/ICMA.2016.7558819",
    "duplicate": false,
    "email": ["jhzhang@bit.edu.cn"],
    "email_new": ["jhzhang@bit.edu.cn"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558819",
    "key": "2d681b43-b383-48fc-9833-1b45c9eaf8e9",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "mri image segmentation by fully convolutional networks",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "2d681b43-b383-48fc-9833-1b45c9eaf8e9.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://www.nitrc.org/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "18",
    "training": 4.0,
    "validation": 0.0,
    "testing": 14.0,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE International Conference on Mechatronics and Automation",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.",
    "authors": "G. Ditzler,  R. Polikar,  G. Rosen",
    "description": "IEEE Transactions on NanoBioscience. 2015",
    "doi": "10.1109/TNB.2015.2461219",
    "duplicate": false,
    "email": [
      "gregory.ditzler@gmail.com",
      "gailr@ece.drexel.edu",
      "polikar@rowan.edu"
    ],
    "email_new": [
      "gregory.ditzler@gmail.com",
      "gailr@ece.drexel.edu",
      "polikar@rowan.edu"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219432",
    "key": "454721f9-3a79-4bdf-9e83-db9f2996e93c",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "multi-layer and recursive neural networks for metagenomic classification",
    "tools": ["torch"],
    "year": 2015.0,
    "id": "454721f9-3a79-4bdf-9e83-db9f2996e93c.pdf",
    "use": 1,
    "domain": "metagenomics",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "dna",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "microorganisms",
    "location_1": "0",
    "method_1": "dbn",
    "method_2": "rnn",
    "method_3": "mlp",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://qiita.ucsd.edu/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "0",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on NanoBioscience",
    "impact_factor": 2.771,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "We propose a new Multi-scale Rotation-invariant Convolutional Neural Network (MRCNN) model for classifying various lung tissue types on high-resolution computed tomography (HRCT). MRCNN employs Gabor-local binary pattern (Gabor-LBP) which introduces a good property in image analysis - invariance to image scales and rotations. In addition, we offer an approach to deal with the problems caused by imbalanced number of samples between different classes in most of the existing works, accomplished by changing the overlapping size between the adjacent patches. Experimental results on a public Interstitial Lung Disease (ILD) database show a superior performance of the proposed method to state-of-the-art.",
    "authors": "Q. Wang,  Y. Zheng,  g. yang,  W. Jin,  X. Chen,  y. yin",
    "description": "IEEE Journal of Biomedical and Health Informatics. 2017",
    "doi": "10.1109/JBHI.2017.2685586",
    "duplicate": false,
    "email": [
      "shdyn2000@mail.sdu.edu.cn",
      "zhengyuanjie@gmail.com",
      "gpyang@sdu.edu.cn",
      "jinweidong@sdufe.edu.cn",
      "xjchen@suda.edu.cn",
      "ylyin@sdu.edu.cn"
    ],
    "email_new": [
      "shdyn2000@mail.sdu.edu.cn",
      "zhengyuanjie@gmail.com",
      "gpyang@sdu.edu.cn",
      "jinweidong@sdufe.edu.cn",
      "xjchen@suda.edu.cn",
      "ylyin@sdu.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883849",
    "key": "7b8ec9d1-e85f-4e80-bc3d-ed9eeb2b225f",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "multi-scale rotation-invariant convolutional neural networks for lung texture classification",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "7b8ec9d1-e85f-4e80-bc3d-ed9eeb2b225f.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "lung",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://medgift.hevs.ch/wordpress/databases/ild-database/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases on longitudinal data has drawn great interest from computer vision researchers. The current state-of-the-art models for many image classification tasks are based on the Convolutional Neural Networks (CNN). However, a key challenge in applying CNN to biological problems is that the available labeled training samples are very limited. Another issue for CNN to be applied in computer aided diagnosis applications is that to achieve better diagnosis and prognosis accuracy, one usually has to deal with the longitudinal dataset, i.e., the dataset of images scanned at different time points. Here we argue that an enhanced CNN model with transfer learning for the joint analysis of tasks from multiple time points or regions of interests may have a potential to improve the accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN based deep learning multi-task dictionary learning framework to address the above challenges. Firstly, we pre-train CNN on the ImageNet dataset and transfer the knowledge from the pre-trained model to the medical imaging progression representation, generating the features for different tasks. Then, we propose a novel unsupervised learning method, termed Multi-task Stochastic Coordinate Coding (MSCC), for learning different tasks by using shared and individual dictionaries and generating the sparse features required to predict the future cognitive clinical scores. We apply our new model in a publicly available neuroimaging cohort to predict clinical measures with two different feature sets and compare them with seven other state-of-the-art methods. The experimental results show our proposed method achieved superior results.",
    "authors":
      "Jie Zhang, Qingyang Li, Richard J. Caselli, Jieping Ye, Yalin Wang",
    "description": "Retrieved from arXiv. 2017",
    "doi": null,
    "duplicate": false,
    "email": [
      "JieZhang.Joena@asu.edu",
      "Qingyang.Li@asu.edu",
      "Yalin.Wang@asu.edu",
      "jpye@umich.edu",
      "caselli.richard@mayo.edu"
    ],
    "email_new": [
      "JieZhang.Joena@asu.edu",
      "Qingyang.Li@asu.edu",
      "Yalin.Wang@asu.edu",
      "jpye@umich.edu",
      "caselli.richard@mayo.edu"
    ],
    "fullUrl": "https://arxiv.org/abs/1709.00042",
    "key": "a05bc5d0-f158-41f2-b8e9-d1cc1c67a9cd",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1709.00042.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "multi-task dictionary learning based convolutional neural network for computer aided diagnosis with longitudinal images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "a05bc5d0-f158-41f2-b8e9-d1cc1c67a9cd.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "regression",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://adni.loni.usc.edu/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automatic diabetes retinopathy (DR) recognition can help DR carriers to receive treatment in early stages and avoid the risk of vision loss. In this paper, we emphasize the role of multiple filter sizes in learning fine-grained discriminant features and propose: (i) two deep convolutional neural networks - Combined Kernels with Multiple Losses Network (CKML Net) and VGGNet with Extra Kernel (VNXK), which are an improvement upon GoogLeNet and VGGNet in context of DR tasks. Learning from existing research, (ii) we propose a hybrid color space, LGI, for DR recognition via proposed nets. (iii) Transfer learning is applied to solve the challenge of imbalanced dataset. The effectiveness of proposed new nets and color space is evaluated using two grand challenge retina datasets: EyePACS and Messidor. Our experimental results show: (iv) CKML Net improves upon GoogLeNet and VNXK improves upon VGGNet on both datasets using the LGI color space. Additionally, proposed methodology improves upon other state of the art results on Messidor dataset for referable/non-referable screening.",
    "authors": "H. H. Vo,  A. Verma",
    "description":
      "2016 IEEE International Symposium on Multimedia (ISM). 2016",
    "doi": "10.1109/ISM.2016.0049",
    "duplicate": false,
    "email": ["hhvo@csu.fullerton.edu", "averma@fullerton.edu"],
    "email_new": ["hhvo@csu.fullerton.edu", "averma@fullerton.edu"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823616",
    "key": "61e88b6c-394f-41be-888d-82cd96c0c4b1",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "new deep neural nets for fine-grained diabetic retinopathy recognition on hybrid color space",
    "tools": ["digits", "caffe"],
    "year": 2016.0,
    "id": "61e88b6c-394f-41be-888d-82cd96c0c4b1.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "fundus photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://www.kaggle.com/c/diabetic-retinopathy-detection",
    "data_link_2":
      "http://www.adcis.net/en/Download-Third-Party/Messidor.htmlindex-en.php",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2016 IEEE International Symposium on Multimedia",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In studies using dental X-ray images, tooth localization is essential to produce accurate results. In this paper, we propose a tooth localization method that tightly localize diverse teeth in periapical dental X-ray images. Oriented tooth proposals are generated by using teeth separation lines (TSLs) and a tooth top line, which are reliable and tight to teeth. To classify each tooth proposal into either a tooth or a non-tooth, we utilize a convolutional neural network (CNN). Our CNN model is trained with three classes, i.e., one negative and two positives, for better classification. In addition, we propose scale based non-maximum suppression by integrating scale confidence with non-maximum suppression to efficiently eliminate multiple tooth localizations.",
    "authors": "H. Eun,  C. Kim",
    "description":
      "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). 2016",
    "doi": "10.1109/APSIPA.2016.7820720",
    "duplicate": false,
    "email": ["hj.eun@kaist.ac.kr", "changick@kaist.ac.kr"],
    "email_new": ["hj.eun@kaist.ac.kr", "changick@kaist.ac.kr"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820720",
    "key": "9f39197c-d7dd-4724-81e6-c1c680348789",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "oriented tooth localization for periapical dental x-ray images via convolutional neural network",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "9f39197c-d7dd-4724-81e6-c1c680348789.pdf",
    "use": 1,
    "domain": "dentistry",
    "task_1": "localization",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "teeth",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Korea",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.",
    "authors": "Hou L, Samaras D, Kurc TM, Gao Y, Davis JE, Saltz JH.",
    "description":
      "Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit.  2016",
    "doi": null,
    "duplicate": false,
    "email": [
      "lehhou@cs.stonybrook.edu",
      "samaras@cs.stonybrook.edu",
      "tahsin.kurc@stonybrook.edu",
      "joel.saltz@stonybrook.edu",
      "yi.gao@stonybrookmedicine.edu",
      "james.davis@stonybrookmedicine.edu"
    ],
    "email_new": [
      "lehhou@cs.stonybrook.edu",
      "samaras@cs.stonybrook.edu",
      "tahsin.kurc@stonybrook.edu",
      "joel.saltz@stonybrook.edu",
      "yi.gao@stonybrookmedicine.edu",
      "james.davis@stonybrookmedicine.edu"
    ],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27795661",
    "key": "dc8439cf-1207-4cd9-b68f-3303b1640887",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "patch-based convolutional neural network for whole slide tissue image classification",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "dc8439cf-1207-4cd9-b68f-3303b1640887.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "wsi",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://portal.gdc.cancer.gov/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE Conference on Computer Vision and Pattern Recognition",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Machine learning analysis of neuroimaging data can accurately predict chronological age in healthy people and deviations from healthy brain ageing have been associated with cognitive impairment and disease. Here we sought to further establish the credentials of brain-predicted age as a biomarker of individual differences in the brain ageing process, using a predictive modelling approach based on deep learning, and specifically convolutional neural networks (CNN), and applied to both pre-processed and raw T1-weighted MRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted age using a large dataset of healthy adults (N = 2001). Next, we sought to establish the heritability of brain-predicted age using a sample of monozygotic and dizygotic female twins (N = 62). Thirdly, we examined the test-retest and multi-centre reliability of brain-predicted age using two samples (within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were generated and compared to a Gaussian Process Regression (GPR) approach, on all datasets. Input data were grey matter (GM) or white matter (WM) volumetric maps generated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted age represents an accurate, highly reliable and genetically-valid phenotype, that has potential to be used as a biomarker of brain ageing. Moreover, age predictions can be accurately generated on raw T1-MRI data, substantially reducing computation time for novel data, bringing the process closer to giving real-time information on brain health in clinical settings.",
    "authors":
      "James H Cole, Rudra PK Poudel, Dimosthenis Tsagkrasoulis, Matthan WA Caan, Claire Steves, Tim D Spector, Giovanni Montana",
    "description": "Retrieved from arXiv. 2016",
    "doi": null,
    "duplicate": false,
    "email": ["giovanni.montana@kcl.ac.uk"],
    "email_new": ["giovanni.montana@kcl.ac.uk"],
    "fullUrl": "https://arxiv.org/abs/1612.02572",
    "key": "9ca4bc2e-ed46-41d6-a24b-b1f2b58a4228",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1612.02572.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker",
    "tools": ["torch"],
    "year": 2016.0,
    "id": "9ca4bc2e-ed46-41d6-a24b-b1f2b58a4228.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "regression",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "1",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "UK",
    "country_2": "Netherlands",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "As a part of the automatic study of visual attention of affected populations with neurodegenerative diseases and to predict whether new gaze records a complaint of these diseases, we should design an automatic model that predicts salient areas in video. Past research showed, that people suffering form dementia are not reactive with regard to degradations on still images. In this paper we study the reaction of healthy normal control subjects on degraded area in videos. Furthermore, in the goal to build an automatic prediction model for salient areas in intentionally degraded videos, we design a deep learning architecture and measure its performances when predicting salient regions on completely unseen data. The obtained results are interesting regarding the reaction of normal control subjects against a degraded area in video.",
    "authors": "S. Chaabouni,  F. Tison,  J. Benois-Pineau,  C. Ben Amar",
    "description":
      "2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI). 2016",
    "doi": "10.1109/CBMI.2016.7500243",
    "duplicate": false,
    "email": [
      "souad.chaabouni@labri.fr",
      "benois-p@labri.fr",
      "francois.tison@chu-bordeaux.fr",
      "chokri.benamar@ieee.org"
    ],
    "email_new": [
      "souad.chaabouni@labri.fr",
      "benois-p@labri.fr",
      "francois.tison@chu-bordeaux.fr",
      "chokri.benamar@ieee.org"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500243",
    "key": "21ca4519-4415-4c97-8d05-a26b35ebefae",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "prediction of visual attention with deep cnn for studies of neurodegenerative diseases",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "21ca4519-4415-4c97-8d05-a26b35ebefae.pdf",
    "use": 1,
    "domain": "neurology",
    "task_1": "saliency",
    "task_2": "0",
    "data_type": "video",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gaze",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.di.ens.fr/~laptev/actions/hollywood2/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 0.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "1707",
    "training": 823.0,
    "validation": 0.0,
    "testing": 884.0,
    "cross_validation": 0.0,
    "country_1": "France",
    "country_2": "Tunisia",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "A novel deep learning architecture (XmasNet) based on convolutional neural networks was developed for the classification of prostate cancer lesions, using the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end training was performed for XmasNet, with data augmentation done through 3D rotation and slicing, in order to incorporate the 3D information of the lesion. XmasNet outperformed traditional machine learning models based on engineered features, for both train and test data. For the test data, XmasNet outperformed 69 methods from 33 participating groups and achieved the second highest AUC (0.84) in the PROSTATEx challenge. This study shows the great potential of deep learning for cancer imaging.",
    "authors": "Saifeng Liu, Huaixiu Zheng, Yesu Feng, Wei Li",
    "description":
      "4 pages, 4 figures, Proc. SPIE 10134, Medical Imaging 2017. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["liusaifeng@gmail.com", "huaixiu.zheng2013@gmail.com"],
    "email_new": ["liusaifeng@gmail.com", "huaixiu.zheng2013@gmail.com"],
    "fullUrl": "https://arxiv.org/abs/1703.04078",
    "key": "f56d6cd2-83ca-421a-96b3-9f521cbda975",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1703.04078.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "prostate cancer diagnosis using deep learning with 3d multiparametric mri",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "f56d6cd2-83ca-421a-96b3-9f521cbda975.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "prostate",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://spie.org/x115569.xml?wt.mc_id=rmi17gb&SSO=1",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "Canada",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren & Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art.",
    "authors": "J. Antony,  K. McGuinness,  N. E. O'Connor,  K. Moran",
    "description":
      "2016 23rd International Conference on Pattern Recognition (ICPR). 2016",
    "doi": "10.1109/ICPR.2016.7899799",
    "duplicate": false,
    "email": ["joseph.antony2@mail.dcu.ie"],
    "email_new": ["joseph.antony2@mail.dcu.ie"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899799",
    "key": "f8ce9555-71a2-46c9-afa5-bfe4c7df1272",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "quantifying radiographic knee osteoarthritis severity using deep convolutional neural networks",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "f8ce9555-71a2-46c9-afa5-bfe4c7df1272.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "regression",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "knee",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "http://www.robots.ox.ac.uk/~vgg/research/very_deep/",
    "based_on_1_name": "vgg_very_deep_convnets",
    "based_on_2":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
    "based_on_2_name": "bvlc_caffenet",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Ireland",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 23rd International Conference on Pattern Recognition (ICPR)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.",
    "authors": "S. Miao,  Z. J. Wang,  Y. Zheng,  R. Liao",
    "description":
      "2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016",
    "doi": "10.1109/ISBI.2016.7493536",
    "duplicate": false,
    "email": ["smiao@ece.ubc.ca", "shun.miao@siemens.com"],
    "email_new": ["smiao@ece.ubc.ca", "shun.miao@siemens.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493536",
    "key": "9770284b-6194-487f-a284-abb5003670b5",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "real-time 2d/3d registration via cnn regression",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "9770284b-6194-487f-a284-abb5003670b5.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "registration",
    "task_2": "0",
    "data_type": "xray",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Canada",
    "country_2": "USA",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.",
    "authors": "A. Fakhry,  T. Zeng,  S. Ji",
    "description": "IEEE Transactions on Medical Imaging. 2017",
    "doi": "10.1109/TMI.2016.2613019",
    "duplicate": false,
    "email": ["afakhry@cs.odu.edu", "tzeng@eecs.wsu.edu", "sji@eecs.wsu.edu"],
    "email_new": [
      "afakhry@cs.odu.edu",
      "tzeng@eecs.wsu.edu",
      "sji@eecs.wsu.edu"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575638",
    "key": "78d046bc-0d81-412d-947f-c29f4d31af74",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "residual deconvolutional networks for brain electron microscopy image segmentation",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "78d046bc-0d81-412d-947f-c29f4d31af74.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "electron microscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "rdn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://brainiac2.mit.edu/SNEMI3D/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2016 IEEE Transactions on Medical Imaging",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Vessel segmentation is a key step for various medical applications. This paper introduces the deep learning architecture to improve the performance of retinal vessel segmentation. Deep learning architecture has been demonstrated having the powerful ability in automatically learning the rich hierarchical representations. In this paper, we formulate the vessel segmentation to a boundary detection problem, and utilize the fully convolutional neural networks (CNNs) to generate a vessel probability map. Our vessel probability map distinguishes the vessels and background in the inadequate contrast region, and has robustness to the pathological regions in the fundus image. Moreover, a fully-connected Conditional Random Fields (CRFs) is also employed to combine the discriminative vessel probability map and long-range interactions between pixels. Finally, a binary vessel segmentation result is obtained by our method. We show that our proposed method achieve a state-of-the-art vessel segmentation performance on the DRIVE and STARE datasets.",
    "authors": "H. Fu,  Y. Xu,  D. W. K. Wong,  J. Liu",
    "description":
      "2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016",
    "doi": "10.1109/ISBI.2016.7493362",
    "duplicate": false,
    "email": [
      "fuhz@i2r.a-star.edu.sg",
      "yaxu@i2r.a-star.edu.sg",
      "wkwong@i2r.a-star.edu.sg",
      "jliu@i2r.a-star.edu.sg"
    ],
    "email_new": [
      "fuhz@i2r.a-star.edu.sg",
      "yaxu@i2r.a-star.edu.sg",
      "wkwong@i2r.a-star.edu.sg",
      "jliu@i2r.a-star.edu.sg"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493362",
    "key": "9ac767ad-078a-4749-ad9b-a424df266751",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "retinal vessel segmentation via deep learning network and fully-connected conditional random fields",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "9ac767ad-078a-4749-ad9b-a424df266751.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "fundus photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "hnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.isi.uu.nl/Research/Databases/DRIVE/",
    "data_link_2": "http://cecas.clemson.edu/~ahoover/stare/",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "https://github.com/s9xie/hed",
    "based_on_1_name": "holistically_nested_edge_detection",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Singapore",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "R-CNN (region-convolutional neural network) has recently achieved very outstanding results in variety of visual detecting fields, and its function of object-proposal-generation can achieve effective training models by using as small samples as possible in the field of machine learning. In this paper, a modified R-CNN is proposed and applied to detect cells under phase contrast microscopy images by adopting multiple object-proposal-generations instead of a single one to extract candidate regions. The results show that the proposed method can obtain better performance than the traditional method by using a single object-proposal-generation.",
    "authors": "F. Deng,  H. Hu,  S. Chen,  Q. Guan,  Y. Zou",
    "description":
      "2015 Sixth International Conference on Intelligent Control and Information Processing (ICICIP). 2015",
    "doi": "10.1109/ICICIP.2015.7388195",
    "duplicate": false,
    "email": ["csy@zjut.edu.cn"],
    "email_new": ["csy@zjut.edu.cn"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388195",
    "key": "2a2d0c01-4d6f-4ee5-9ecd-7652866e82ac",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "rich feature hierarchies for cell detecting under phase contrast microscopy images",
    "tools": ["caffe"],
    "year": 2015.0,
    "id": "2a2d0c01-4d6f-4ee5-9ecd-7652866e82ac.pdf",
    "use": 1,
    "domain": "microscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "phase contrast microscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "0",
    "location_1": "0",
    "method_1": "regioncnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "100",
    "training": 80.0,
    "validation": 0.0,
    "testing": 20.0,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2015 Sixth International Conference on Intelligent Control and Information Processing (ICICIP)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "We present a deep learning based framework, called ROSE, to accurately predict ribosome stalling events in translation elongation from coding sequences based on high-throughput ribosome profiling data. Our validation results demonstrate the superior performance of ROSE over conventional prediction models. ROSE provides an effective index to estimate the likelihood of translational pausing at codon resolution and understand diverse putative regulatory factors of ribosome stalling. Also, the ribosome stalling landscape computed by ROSE can recover the functional interplay between ribosome stalling and cotranslational events in protein biogenesis, including protein targeting by the signal recognition particle (SRP) and protein secondary structure formation.",
    "authors":
      "Sai Zhang, Hailin Hu, Jingtian Zhou, Xuan He, Tao Jiang, Jianyang Zeng",
    "description": "Retrieved from bioRxiv. 2016",
    "doi": null,
    "duplicate": false,
    "email": ["zengjy321@tsinghua.edu.cn"],
    "email_new": ["zengjy321@tsinghua.edu.cn"],
    "fullUrl": "http://www.biorxiv.org/content/early/2016/11/15/067108",
    "key": "e960c273-a280-41cc-938a-63d30f3c31ea",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "http://www.biorxiv.org/content/early/2016/11/15/067108.full.pdf",
    "phase": 1,
    "received": 0,
    "source": "biorxiv",
    "title":
      "rose: a deep learning based framework for predicting ribosome stalling",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "e960c273-a280-41cc-938a-63d30f3c31ea.pdf",
    "use": 1,
    "domain": "genomics",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "sequence",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "cell",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.uniprot.org/",
    "data_link_2": "https://gwips.ucc.ie/",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "China",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "PURPOSE: Accurate segmentation of organs-at-risks (OARs) is the key step for efficient planning of radiation therapy for head and neck (HaN) cancer treatment. In the work, we proposed the first deep learning-based algorithm, for segmentation of OARs in HaN CT images, and compared its performance against state-of-the-art automated segmentation algorithms, commercial software, and interobserver variability. METHODS: Convolutional neural networks (CNNs)-a concept from the field of deep learning-were used to study consistent intensity patterns of OARs from training CT images and to segment the OAR in a previously unseen test CT image. For CNN training, we extracted a representative number of positive intensity patches around voxels that belong to the OAR of interest in training CT images, and negative intensity patches around voxels that belong to the surrounding structures. These patches then passed through a sequence of CNN layers that captured local image features such as corners, end-points, and edges, and combined them into more complex high-order features that can efficiently describe the OAR. The trained network was applied to classify voxels in a region of interest in the test image where the corresponding OAR is expected to be located. We then smoothed the obtained classification results by using Markov random fields algorithm. We finally extracted the largest connected component of the smoothed voxels classified as the OAR by CNN, performed dilate-erode operations to remove cavities of the component, which resulted in segmentation of the OAR in the test image. RESULTS: The performance of CNNs was validated on segmentation of spinal cord, mandible, parotid glands, submandibular glands, larynx, pharynx, eye globes, optic nerves, and optic chiasm using 50 CT images. The obtained segmentation results varied from 37.4% Dice coefficient (DSC) for chiasm to 89.5% DSC for mandible. We also analyzed the performance of state-of-the-art algorithms and commercial software reported in the literature, and observed that CNNs demonstrate similar or superior performance on segmentation of spinal cord, mandible, parotid glands, larynx, pharynx, eye globes, and optic nerves, but inferior performance on segmentation of submandibular glands and optic chiasm. CONCLUSION: We concluded that convolution neural networks can accurately segment most of OARs using a representative database of 50 HaN CT images. At the same time, inclusion of additional information, for example, MR images, may be beneficial to some OARs with poorly visible boundaries. \u00a9 2016 American Association of Physicists in Medicine.",
    "authors": "Ibragimov B, Xing L.",
    "description": "Med Phys.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["bulat@stanford.edu"],
    "email_new": ["bulat@stanford.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28205307",
    "key": "a274c952-6989-478c-bd5c-47dd0902015f",
    "keyok": 1,
    "keywords":
      "convolutional neural networks; deep learning; head and neck; radiotherapy; segmentation",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "segmentation of organs-at-risks in head and neck ct images using convolutional neural networks",
    "tools": ["cntk"],
    "year": 2017.0,
    "id": "a274c952-6989-478c-bd5c-47dd0902015f.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "medical physics",
    "impact_factor": 2.635,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The accurate segmentation of pulmonary CT images is of great significance to clinical computer-aided diagnosis and treatment. In order to avoid the explicit extraction of image features and improve the efficiency of image segmentation and reduce the influence of human factors on the segmentation results, this paper proposes a method of segmenting pulmonary CT image based on membership function convolution neural network (MFCNN). First, the method uses pulmonary CT image filtered by the Gaussian as the input data of the convolution neural network. Then, that uses the improved convolution neural network to achieve the initial segmentation of the image. Finally, the final segmentation result is obtained by setting the threshold based on this paper method. After experimental comparison, this paper demonstrates the feasibility and effectiveness of convolution neural network in the segmentation of pulmonary CT images.",
    "authors": "J. Xu,  H. Liu",
    "description":
      "2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC). 2017",
    "doi": "10.1109/CSE-EUC.2017.42",
    "duplicate": false,
    "email": ["xxxuuujjjuuunnn@126.com", "liuh_lh@126.com"],
    "email_new": ["xxxuuujjjuuunnn@126.com", "liuh_lh@126.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005794",
    "key": "e9a0a42f-0c1e-4eb9-bd87-9f81126e8bfe",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "segmentation of pulmonary ct image by using convolutional neural network based on membership function",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "e9a0a42f-0c1e-4eb9-bd87-9f81126e8bfe.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "lung",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt",
    "based_on_1_name": "bvlc_lenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400[Formula: see text]000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to &gt; 0.99) and accuracy of classification (up to &gt; 0.97 ). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity &gt; 0.87 ), and fares well on pathological cases.",
    "authors": "Liskowski P, Krawiec K.",
    "description": "IEEE Trans Med Imaging.  2016",
    "doi": null,
    "duplicate": false,
    "email": ["pliskowski@cs.put.poznan.pl", "kkrawiec@cs.put.poznan.pl"],
    "email_new": ["pliskowski@cs.put.poznan.pl", "kkrawiec@cs.put.poznan.pl"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27046869",
    "key": "d13ef99d-bf6b-45de-b094-420da3a2e7ab",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title": "segmenting retinal blood vessels with deep neural networks",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "d13ef99d-bf6b-45de-b094-420da3a2e7ab.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "fundus photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.isi.uu.nl/Research/Databases/DRIVE/",
    "data_link_2": "http://cecas.clemson.edu/~ahoover/stare/",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "http://www.cs.put.poznan.pl/dltmi",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Poland",
    "country_2": "0",
    "country_3": "0",
    "journal": "IEEE Transactions on Medical Imaging",
    "impact_factor": 3.942,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Low Dose Computed tomography (CT) has offered tremendous benefits in radiation restricted applications, but the quantum noise as resulted by the insufficient number of photons could potentially harm the diagnostic performance. Current image-based denoising methods tend to produce a blur effect on the final reconstructed results especially in high noise levels. In this paper, a deep learning based approach was proposed to mitigate this problem. An adversarially trained network and a sharpness detection network were trained to guide the training process. Experiments on both simulated and real dataset shows that the results of the proposed method have very small resolution loss and achieves better performance relative to the-state-of-art methods both quantitatively and visually.",
    "authors": "Xin Yi, Paul Babyn",
    "description": "Retrieved from arXiv. 2017",
    "doi": null,
    "duplicate": false,
    "email": ["xiy525@mail.usask.ca"],
    "email_new": ["xiy525@mail.usask.ca"],
    "fullUrl": "https://arxiv.org/abs/1708.06453",
    "key": "972e8f62-0a96-4c59-b7b9-89f4ba0212ad",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1708.06453.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "sharpness-aware low dose ct denoising using conditional generative adversarial network",
    "tools": ["digits", "torch"],
    "year": 2017.0,
    "id": "972e8f62-0a96-4c59-b7b9-89f4ba0212ad.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "denoising",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "gan",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "https://imaging.nci.nih.gov/ncia/login.jsf",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/",
    "based_on_1_name": "unet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Canada",
    "country_2": "0",
    "country_3": "0",
    "journal": "Journal of Digital Imaging",
    "impact_factor": 1.407,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Vessel segmentation of digital retinal images plays an important role in diagnosis of diseases such as diabetics, hypertension and retinopathy of prematurity due to these diseases impact the retina. In this paper, a novel Size-Invariant Fully Convolutional Neural Network (SIFCN) is proposed to address the automatic retinal vessel segmentation problems. The input data of the network is the patches of images and the corresponding pixel-wise labels. A consecutive convolution layers and pooling layers follow the input data, so that the network can learn the abstract features to segment retinal vessel. Our network is designed to hold the height and width of data of each layer with padding and assign pooling stride so that the spatial information maintain and up-sample is not required. Compared with the pixel-wise retinal vessel segmentation approaches, our patch-wise segmentation is much more efficient since in each cycle it can predict all the pixels of the patch. Our overlapped SIFCN approach achieves accuracy of 0.9471, with the AUC of 0.9682. And our non-overlap SIFCN is the most efficient approach among the deep learning approaches, costing only 3.68 seconds per image, and the overlapped SIFCN costs 31.17 seconds per image.",
    "authors": "Y. Luo,  H. Cheng,  L. Yang",
    "description":
      "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). 2016",
    "doi": "10.1109/APSIPA.2016.7820677",
    "duplicate": false,
    "email": [
      "1099361041@qq.com",
      "hcheng@uestc.edu.cn",
      "yanglu@uestc.edu.cn"
    ],
    "email_new": [
      "1099361041@qq.com",
      "hcheng@uestc.edu.cn",
      "yanglu@uestc.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820677",
    "key": "4c314e2b-7f3b-437e-bf24-b341ea4dbe84",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "size-invariant fully convolutional neural network for vessel segmentation of digital retinal images",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "4c314e2b-7f3b-437e-bf24-b341ea4dbe84.pdf",
    "use": 1,
    "domain": "ophthalmology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "fundus photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "eye",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.isi.uu.nl/Research/Databases/DRIVE/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "A baby's gestational age determines whether or not they are preterm, which helps clinicians decide on suitable post-natal treatment. The most accurate dating methods use Ultrasound Scan (USS) machines, but these machines are expensive, require trained personnel and cannot always be deployed to remote areas. In the absence of USS, the Ballard Score can be used, which is a manual postnatal dating method. However, this method is highly subjective and results can vary widely depending on the experience of the rater. In this paper, we present an automatic system for postnatal gestational age estimation aimed to be deployed on mobile phones, using small sets of images of a newborn's face, foot and ear. We present a novel two-stage approach that makes the most out of Convolutional Neural Networks trained on small sets of images to predict broad classes of gestational age, and then fuse the outputs of these discrete classes with a baby's weight to make fine-grained predictions of gestational age. On a purpose=collected dataset of 88 babies, experiments show that our approach attains an expected error of 6 days and is three times more accurate than the manual postnatal method (Ballard). Making use of images improves predictions by 30% compared to using weight only. This indicates that even with a very small set of data, our method is a viable candidate for postnatal gestational age estimation in areas were USS is not available.",
    "authors": "M. T. Torres,  M. F. Valstar,  C. Henry,  C. Ward,  D. Sharkey",
    "description":
      "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). 2017",
    "doi": "10.1109/FG.2017.19",
    "duplicate": false,
    "email": ["michel.valstar@nottingham.ac.uk"],
    "email_new": ["michel.valstar@nottingham.ac.uk"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961726",
    "key": "351c16ad-87b3-462b-a38f-eca2ae400dd6",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "small sample deep learning for newborn gestational age estimation",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "351c16ad-87b3-462b-a38f-eca2ae400dd6.pdf",
    "use": 1,
    "domain": "gestation",
    "task_1": "segmentation",
    "task_2": "regression",
    "data_type": "photographic",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "infant",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "cnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "https://github.com/shelhamer/fcn.berkeleyvision.org",
    "based_on_1_name": "bvlc_fully_convolutional_nets",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": "88",
    "training": 0.0,
    "validation": 0.0,
    "testing": 0.0,
    "cross_validation": 1.0,
    "country_1": "UK",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of \u201cDeep Learning\u201d strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 \u00d7 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.",
    "authors":
      "J. Xu,  L. Xiang,  Q. Liu,  H. Gilmore,  J. Wu,  J. Tang,  A. Madabhushi",
    "description": "IEEE Transactions on Medical Imaging. 2016",
    "doi": "10.1109/TMI.2015.2458702",
    "duplicate": false,
    "email": ["axm788@case.edu", "xujung@gmail.com"],
    "email_new": ["axm788@case.edu", "xujung@gmail.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163353",
    "key": "77c59385-02aa-4329-a9d1-9b80483efae0",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "stacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "77c59385-02aa-4329-a9d1-9b80483efae0.pdf",
    "use": 1,
    "domain": "pathology",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "histapathology",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "0",
    "method_1": "ssae",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://engineering.case.edu/centers/ccipd/data",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 1.0,
    "code_link": "http://engineering.case.edu/centers/ccipd/data",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "USA",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2015 IEEE Transactions on Medical Imaging",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.",
    "authors":
      "H. Chen,  D. Ni,  J. Qin,  S. Li,  X. Yang,  T. Wang,  P. A. Heng",
    "description": "IEEE Journal of Biomedical and Health Informatics. 2015",
    "doi": "10.1109/JBHI.2015.2425041",
    "duplicate": false,
    "email": [
      "jackie.haochen@gmail.com",
      "nidong@szu.edu.cn",
      "jqin@szu.edu.cn",
      "lishengli63@126.com",
      "pheng@cse.cuhk.edu.hk"
    ],
    "email_new": [
      "jackie.haochen@gmail.com",
      "nidong@szu.edu.cn",
      "jqin@szu.edu.cn",
      "lishengli63@126.com",
      "pheng@cse.cuhk.edu.hk"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943",
    "key": "2f4ae966-7c74-4fb8-923e-301ac4d845fa",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "standard plane localization in fetal ultrasound via domain transferred deep neural networks",
    "tools": ["caffe"],
    "year": 2015.0,
    "id": "2f4ae966-7c74-4fb8-923e-301ac4d845fa.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "plane detection",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "fetal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
    "based_on_1_name": "bvlc_caffenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": "519",
    "training": 300.0,
    "validation": 0.0,
    "testing": 219.0,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "Hong Kong",
    "country_3": "0",
    "journal": "IEEE Journal of Biomedical and Health Informatics",
    "impact_factor": 3.451,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Super-resolution (SR) of single image is a meaningful challenge in medical images based diagnosis, while the image resolution is limited. Also, numerous deep neural networks based models were proposed and achieve excellent performance which is superior to the previous handcrafted methods. In this paper, we employ a deep convolutional neural networks for the super-resolution (SR) of single medical image, which learns the nonlinear mapping from the low-resolution space to high-resolution space directly. In addition, we use three sets imaging data (Mammary gland, Prostate tissue and Human brain) training deep network respectively. Firstly, we use Randomized Rectified Linear Unit (RReLU), which incorporates a nonzero slope for negative part to solve the problem of over compression. Secondly, for the purpose of enhancing the quality of reconstructed result and reducing the noise of over-fitting, Nesterov's Accelerated Gradient (NAG) method on the SRCNN is used to accelerate the convergence of loss function and avoid the large oscillations. A comparative performance evaluation is carried out over a set of experiments using real imaging data to verify the validity of proposed algorithm.",
    "authors": "X. Yang,  S. Zhant,  C. Hu,  Z. Liang,  D. Xie",
    "description":
      "2016 8th International Conference on Wireless Communications & Signal Processing (WCSP). 2016",
    "doi": "10.1109/WCSP.2016.7752617",
    "duplicate": false,
    "email": ["shu_zhan@hfut.edu.cn"],
    "email_new": ["shu_zhan@hfut.edu.cn"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752617",
    "key": "749fd3f7-ddb7-4bd2-b802-00b81963eef1",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title": "super-resolution of medical image using representation learning",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "749fd3f7-ddb7-4bd2-b802-00b81963eef1.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "super resolution",
    "task_2": "0",
    "data_type": "ct",
    "data_type_1": "mri",
    "data_type_2": "0",
    "location": "breast",
    "location_1": "prostate",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 8th International Conference on Wireless Communications & Signal Processing",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "With many thyroid nodules being incidentally detected, it is important to identify as many malignant nodules as possible while excluding those that are highly likely to be benign from fine needle aspiration (FNA) biopsies or surgeries. This paper presents a computer-aided diagnosis (CAD) system for classifying thyroid nodules in ultrasound images. We use deep learning approach to extract features from thyroid ultrasound images. Ultrasound images are pre-processed to calibrate their scale and remove the artifacts. A pre-trained GoogLeNet model is then fine-tuned using the pre-processed image samples which leads to superior feature extraction. The extracted features of the thyroid ultrasound images are sent to a Cost-sensitive Random Forest classifier to classify the images into 'malignant' and 'benign' cases. The experimental results show the proposed fine-tuned GoogLeNet model achieves excellent classification performance, attaining 98.29% classification accuracy, 99.10% sensitivity and 93.90% specificity for the images in an open access database (Pedraza et al. 16), while 96.34% classification accuracy, 86% sensitivity and 99% specificity for the images in our local health region database.",
    "authors": "Chi J, Walia E, Babyn P, Wang J, Groot G, Eramian M.",
    "description": "J Digit Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["chi.jianning@gmail.com"],
    "email_new": ["chi.jianning@gmail.com"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/28695342",
    "key": "e42f4303-10cc-4ed4-ae84-f9c43037c5c8",
    "keyok": 1,
    "keywords":
      "Computer vision; Convolutional neural network; Deep learning; Fine-tuning; Machine learning; Thyroid nodules; Ultrasonography",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "thyroid nodule classification in ultrasound images by fine-tuning deep convolutional neural network",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "e42f4303-10cc-4ed4-ae84-f9c43037c5c8.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "thyroid",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 1.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "http://www.cimalab.unal.edu.co/",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
    "based_on_1_name": "bvlc_googlelenet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Canada",
    "country_2": "0",
    "country_3": "0",
    "journal": "Journal of Digital Imaging",
    "impact_factor": 1.407,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Tongue diagnosis is one of the most important parts in \u201cinspection diagnosis\u201d of Traditional Chinese Medicine (TCM). Observing tongue shape can help to understand the changes in human body and thereby to estimate the illness. This paper presents a method of recognizing tongue shapes based on Convolution Neural Network. The proposed method enhances the features of tongue images with preprocessing to ensure the data suitable for tongue shape binary classification. In view of the special texture and outline of tongue, the whole tongue images of dot-sting tongue and fissured tongue is transformed by Gabor filter, and the tooth-marked are processed by boundary detection approach. CNN is adopted because it has achieved remarkable results in computer vision and pattern recognition, and the model training through neural network coincides with the Chinese medicine dialectics through experience. Based on commonly used Alex-net, network is optimized with batch normalization to improve efficiency. The experimental results indicate that the preprocessing methods increase the accuracy and decreases the time of training process of tongue shape classification, which proves that the method is effective for the recognition of different tongue shapes.",
    "authors":
      "C. M. Huo,  H. Zheng,  H. Y. Su,  Z. L. Sun,  Y. J. Cai,  Y. F. Xu",
    "description":
      "2017 2nd Asia-Pacific Conference on Intelligent Robot Systems (ACIRS). 2017",
    "doi": "10.1109/ACIRS.2017.7986062",
    "duplicate": false,
    "email": [
      "1874111766@qq.com",
      "sapphire@bit.edu.cn",
      "hongzheng@bit.edu.cn",
      "henrysu@bit.edu.cn",
      "sunzl8417@163.com",
      "2120160980@bit.edu.cn"
    ],
    "email_new": [
      "1874111766@qq.com",
      "sapphire@bit.edu.cn",
      "hongzheng@bit.edu.cn",
      "henrysu@bit.edu.cn",
      "sunzl8417@163.com",
      "2120160980@bit.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986062",
    "key": "ec3f15f0-6c20-4e9d-a864-f8ce2568fd1b",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "tongue shape classification integrating image preprocessing and convolution neural network",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "ec3f15f0-6c20-4e9d-a864-f8ce2568fd1b.pdf",
    "use": 1,
    "domain": "tcm",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "photography",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "tongue",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 2nd Asia-Pacific Conference on Intelligent Robot Systems",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The purpose of this study is to evaluate transfer learning with deep convolutional neural networks for the classification of abdominal ultrasound images. Grayscale images from 185 consecutive clinical abdominal ultrasound studies were categorized into 11 categories based on the text annotation specified by the technologist for the image. Cropped images were rescaled to 256\u2009\u00d7\u2009256 resolution and randomized, with 4094 images from 136 studies constituting the training set, and 1423 images from 49 studies constituting the test set. The fully connected layers of two convolutional neural networks based on CaffeNet and VGGNet, previously trained on the 2012 Large Scale Visual Recognition Challenge data set, were retrained on the training set. Weights in the convolutional layers of each network were frozen to serve as fixed feature extractors. Accuracy on the test set was evaluated for each network. A radiologist experienced in abdominal ultrasound also independently classified the images in the test set into the same 11 categories. The CaffeNet network classified 77.3% of the test set images accurately (1100/1423 images), with a top-2 accuracy of 90.4% (1287/1423 images). The larger VGGNet network classified 77.9% of the test set accurately (1109/1423 images), with a top-2 accuracy of VGGNet was 89.7% (1276/1423 images). The radiologist classified 71.7% of the test set images correctly (1020/1423 images). The differences in classification accuracies between both neural networks and the radiologist were statistically significant (p\u2009&lt;\u20090.001). The results demonstrate that transfer learning with convolutional neural networks may be used to construct effective classifiers for abdominal ultrasound images.",
    "authors": "Cheng PM, Malhi HS.",
    "description": "J Digit Imaging.  2017",
    "doi": null,
    "duplicate": false,
    "email": ["phillip.cheng@med.usc.edu"],
    "email_new": ["phillip.cheng@med.usc.edu"],
    "fullUrl": "https://www.ncbi.nlm.nih.gov//pubmed/27896451",
    "key": "0dffbac9-2cdf-45f3-9199-dfdeb51b6b2f",
    "keyok": 1,
    "keywords":
      "Artificial neural networks; Classification; Deep learning; Digital image processing; Machine learning",
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "pubmed",
    "title":
      "transfer learning with convolutional neural networks for classification of abdominal ultrasound images",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "0dffbac9-2cdf-45f3-9199-dfdeb51b6b2f.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "abdomin",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet",
    "based_on_1_name": "bvlc_caffenet",
    "based_on_2": "https://gist.github.com/ksimonyan/211839e770f7b538e2d8",
    "based_on_2_name": "vgg_very_deep_convnets",
    "transfer_learning": 1.0,
    "total": "185",
    "training": 136.0,
    "validation": 0.0,
    "testing": 49.0,
    "cross_validation": 0.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "Journal of Digital Imaging",
    "impact_factor": 1.407,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.",
    "authors":
      "H. Chen,  L. Wu,  Q. Dou,  J. Qin,  S. Li,  J. Z. Cheng,  D. Ni,  P. A. Heng",
    "description": "IEEE Transactions on Cybernetics. 2017",
    "doi": "10.1109/TCYB.2017.2685080",
    "duplicate": false,
    "email": [
      "jackie.haochen@gmail.com",
      "jzcheng@szu.edu.cn",
      "nidong@szu.edu.cn"
    ],
    "email_new": [
      "jackie.haochen@gmail.com",
      "jzcheng@szu.edu.cn",
      "nidong@szu.edu.cn"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890445",
    "key": "6240cc26-0b20-4dc5-ae04-1085c9723f53",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "ultrasound standard plane detection using a composite neural network framework",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "6240cc26-0b20-4dc5-ae04-1085c9723f53.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "plane detection",
    "task_2": "0",
    "data_type": "us",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "fetal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "rnn",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": null,
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "China",
    "country_2": "Hong Kong",
    "country_3": "0",
    "journal": "IEEE Transactions on Cybernetics",
    "impact_factor": 7.384,
    "conference": "0",
    "public": 0.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of Google Search and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for joint mining of deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized convergence of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75:3%, compared to the state-of-the-art supervised classification accuracy of 81:0% (when both are based on the VGG-VD model).",
    "authors":
      "X. Wang,  L. Lu,  H. C. Shin,  L. Kim,  M. Bagheri,  I. Nogues,  J. Yao,  R. M. Summers",
    "description":
      "2017 IEEE Winter Conference on Applications of Computer Vision (WACV). 2017",
    "doi": "10.1109/WACV.2017.116",
    "duplicate": false,
    "email": [
      "xiaosong.wang@nih.gov",
      "le.lu@nih.gov",
      "lauren.kim2@nih.gov",
      "mohammad.bagheri@nih.gov",
      "isabella.nogues@nih.gov",
      "jyao@cc.nih.gov",
      "rms@nih.gov"
    ],
    "email_new": [
      "xiaosong.wang@nih.gov",
      "le.lu@nih.gov",
      "lauren.kim2@nih.gov",
      "mohammad.bagheri@nih.gov",
      "isabella.nogues@nih.gov",
      "jyao@cc.nih.gov",
      "rms@nih.gov"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926699",
    "key": "637c6bb6-95e7-4dd3-b984-6e0143b6f516",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "unsupervised joint mining of deep features and image labels for large-scale radiology image categorization and scene recognition",
    "tools": ["caffe"],
    "year": 2017.0,
    "id": "637c6bb6-95e7-4dd3-b984-6e0143b6f516.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "classification",
    "task_2": "0",
    "data_type": "text",
    "data_type_1": "ct",
    "data_type_2": "mri",
    "location": "multi-organ",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 0.0,
    "data_shared": 0.0,
    "data_link_1": "0",
    "data_link_2": "0",
    "data_ok": 0.0,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": 0.0,
    "pseudocode": null,
    "based_on_1":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet",
    "based_on_1_name": "bvlc_alexnet",
    "based_on_2":
      "https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet",
    "based_on_2_name": "bvlc_googlelenet",
    "transfer_learning": 1.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "USA",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2017 IEEE Winter Conference on Applications of Computer Vision",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",
    "authors": "F. Milletari,  N. Navab,  S. A. Ahmadi",
    "description":
      "2016 Fourth International Conference on 3D Vision (3DV). 2016",
    "doi": "10.1109/3DV.2016.79",
    "duplicate": false,
    "email": [
      "fausto.milletari@tum.de",
      "navab@cs.tum.edu",
      "ahmad.ahmadi@med.uni-muenchen.de"
    ],
    "email_new": [
      "fausto.milletari@tum.de",
      "navab@cs.tum.edu",
      "ahmad.ahmadi@med.uni-muenchen.de"
    ],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785132",
    "key": "4c992cf9-376f-4048-977a-2eba0a611c86",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "v-net: fully convolutional neural networks for volumetric medical image segmentation",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "4c992cf9-376f-4048-977a-2eba0a611c86.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "prostate",
    "location_1": "0",
    "method_1": "fullycnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "https://promise12.grand-challenge.org/download/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 1.0,
    "code_link": "https://github.com/faustomilletari/VNet",
    "code_ok": 1.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Germany",
    "country_2": "0",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference": "2016 Fourth International Conference on 3D Vision",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Recently deep residual learning with residual units for training very deep neural networks advanced the state-of-the-art performance on 2D image recognition tasks, e.g., object detection and segmentation. However, how to fully leverage contextual representations for recognition tasks from volumetric data has not been well studied, especially in the field of medical image computing, where a majority of image modalities are in volumetric format. In this paper we explore the deep residual learning on the task of volumetric brain segmentation. There are at least two main contributions in our work. First, we propose a deep voxelwise residual network, referred as VoxResNet, which borrows the spirit of deep residual learning in 2D image recognition tasks, and is extended into a 3D variant for handling volumetric data. Second, an auto-context version of VoxResNet is proposed by seamlessly integrating the low-level image appearance features, implicit shape information and high-level context together for further improving the volumetric segmentation performance. Extensive experiments on the challenging benchmark of brain segmentation from magnetic resonance (MR) images corroborated the efficacy of our proposed method in dealing with volumetric data. We believe this work unravels the potential of 3D deep learning to advance the recognition performance on volumetric image segmentation.",
    "authors": "Hao Chen, Qi Dou, Lequan Yu, Pheng-Ann Heng",
    "description": "Retrieved from arXiv. 2016",
    "doi": null,
    "duplicate": false,
    "email": ["hchen@cse.cuhk.edu.hk"],
    "email_new": ["hchen@cse.cuhk.edu.hk"],
    "fullUrl": "https://arxiv.org/abs/1608.05895",
    "key": "6054d13e-6f9a-406f-a4d8-1f2d4be89100",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": "https://arxiv.org/pdf/1608.05895.pdf",
    "phase": 1,
    "received": 0,
    "source": "arxiv",
    "title":
      "voxresnet: deep voxelwise residual networks for volumetric brain segmentation",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "6054d13e-6f9a-406f-a4d8-1f2d4be89100.pdf",
    "use": 1,
    "domain": "radiology",
    "task_1": "segmentation",
    "task_2": "0",
    "data_type": "mri",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "brain",
    "location_1": "0",
    "method_1": "rcnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "http://mrbrains13.isi.uu.nl/",
    "data_link_2": "0",
    "data_ok": 1.0,
    "code_public": 1.0,
    "code_link": "http://appsrv.cse.cuhk.edu.hk/~hchen/research/seg_brain.html",
    "code_ok": 1.0,
    "pseudocode": 1.0,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 1.0,
    "country_1": "Hong Kong",
    "country_2": "0",
    "country_3": "0",
    "journal": "NeuroImage",
    "impact_factor": 5.835,
    "conference": "0",
    "public": 1.0,
    "Unnamed: 40": null
  },
  {
    "abstract":
      "Graphic image annotations provide the necessary ground truth information for supervised machine learning in image-based computer-aided medical diagnosis. Performing such annotations is usually a time-consuming and cost-inefficient process requiring knowledge from domain experts. To cope with this problem we propose a novel weakly-supervised learning method based on a Convolutional Neural Network (CNN) architecture. The advantage of the proposed method over conventional supervised approaches is that only image-level semantic annotations are used in the training process, instead of pixel-level graphic annotations. This can drastically reduce the required annotation effort. Its advantage over the few state-of-the-art weakly-supervised CNN architectures is its simplicity. The performance of the proposed method is evaluated in the context of computer-aided detection of inflammatory gastrointestinal lesions in wireless capsule endoscopy videos. This is a broad category of lesions, for which early detection and treatment can be of vital importance. The results show that the proposed weakly-supervised learning method can be more effective than the conventional supervised learning, with an accuracy of 90%.",
    "authors":
      "S. V. Georgakopoulos,  D. K. Iakovidis,  M. Vasilakakis,  V. P. Plagianakos,  A. Koulaouzidis",
    "description":
      "2016 IEEE International Conference on Imaging Systems and Techniques (IST). 2016",
    "doi": "10.1109/IST.2016.7738279",
    "duplicate": false,
    "email": ["spirosgeorg@gmail.com"],
    "email_new": ["spirosgeorg@gmail.com"],
    "fullUrl": "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738279",
    "key": "874abeb2-84e8-420e-b9af-55bfcf231116",
    "keyok": 1,
    "keywords": null,
    "pdfUrl": null,
    "phase": 1,
    "received": 0,
    "source": "ieee",
    "title":
      "weakly-supervised convolutional learning for detection of inflammatory gastrointestinal lesions",
    "tools": ["caffe"],
    "year": 2016.0,
    "id": "874abeb2-84e8-420e-b9af-55bfcf231116.pdf",
    "use": 1,
    "domain": "endoscopy",
    "task_1": "detection",
    "task_2": "0",
    "data_type": "wireless capsule endoscopy",
    "data_type_1": "0",
    "data_type_2": "0",
    "location": "gastrointestinal",
    "location_1": "0",
    "method_1": "cnn",
    "method_2": "0",
    "method_3": "0",
    "data_public": 0.0,
    "data_upon_request": 1.0,
    "data_shared": 0.0,
    "data_link_1": "https://is-innovation.eu/kid/register.php",
    "data_link_2": "0",
    "data_ok": null,
    "code_public": 0.0,
    "code_link": "0",
    "code_ok": null,
    "pseudocode": null,
    "based_on_1": "0",
    "based_on_1_name": "0",
    "based_on_2": "0",
    "based_on_2_name": "0",
    "transfer_learning": 0.0,
    "total": null,
    "training": null,
    "validation": null,
    "testing": null,
    "cross_validation": 0.0,
    "country_1": "Greece",
    "country_2": "UK",
    "country_3": "0",
    "journal": "0",
    "impact_factor": 0.0,
    "conference":
      "2016 IEEE International Conference on Imaging Systems and Techniques (IST)",
    "public": 0.0,
    "Unnamed: 40": null
  }
  ];
  export default data;
